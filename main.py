# -*- coding: utf-8 -*-
"""main.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1vR143zafEXBFtkgRE8fP1jM62b51w94q

# Multi-Person 3D Pose Estimation for Pistol Shooting & Martial Arts Training

This notebook implements advanced multi-person 3D pose estimation using multiple AI models for robust tracking in pistol shooting and martial arts training scenarios.

## Features:
- **Multi-person tracking** (2-3 people simultaneously)
- **3D pose estimation** with depth information
- **Stable tracking** with temporal smoothing
- **Hidden body part detection** using advanced models
- **Real-time processing** capabilities
- **Multiple model support** (MediaPipe, OpenPose, etc.)

## Models Used:
1. **MediaPipe Pose** - Fast and accurate 2D/3D pose estimation
2. **OpenPose** - Robust multi-person detection
3. **PoseNet** - Lightweight alternative
4. **Custom temporal filters** - For stability and smoothness
"""

# Install required packages for Google Colab
# Reinstalling mediapipe, tensorflow, and scipy with compatible versions to resolve numpy conflict
# !pip install mediapipe==0.10.21 tensorflow==2.19.0 scipy==1.13.1 opencv-python numpy matplotlib scikit-learn
# !pip install tensorflow-hub
# !pip install torch torchvision
# !pip install ultralytics
# !pip install supervision

# # For OpenPose (alternative approach)
# !pip install opencv-contrib-python

# # Additional packages for 3D visualization
# !pip install plotly
# !pip install trimesh

print("✅ All packages installed successfully!")

# Import all necessary libraries
import cv2
import numpy as np  # Already handled by mediapipe/tensorflow install
import mediapipe as mp
import matplotlib.pyplot as plt
import matplotlib.patches as patches
from matplotlib.animation import FuncAnimation
import plotly.graph_objects as go
import plotly.express as px
from plotly.subplots import make_subplots
import json
import time
from collections import deque
from scipy import signal
from scipy.spatial.distance import cdist
from sklearn.cluster import DBSCAN
import warnings
warnings.filterwarnings('ignore')

# TensorFlow and TensorFlow Hub for additional models
import tensorflow as tf
import tensorflow_hub as hub

# PyTorch for advanced models
import torch
import torch.nn as nn

print("✅ All libraries imported successfully!")
print(f"OpenCV version: {cv2.__version__}")
print(f"MediaPipe version: {mp.__version__}")
print(f"TensorFlow version: {tf.__version__}")
print(f"PyTorch version: {torch.__version__}")

# Pose estimation configuration and constants
class PoseConfig:
    # MediaPipe settings
    MEDIAPIPE_MODEL_COMPLEXITY = 2  # 0, 1, or 2 (higher = more accurate)
    MEDIAPIPE_SMOOTH_LANDMARKS = True
    MEDIAPIPE_ENABLE_SEGMENTATION = False
    MEDIAPIPE_SMOOTH_SEGMENTATION = True
    MEDIAPIPE_MIN_DETECTION_CONFIDENCE = 0.5
    MEDIAPIPE_MIN_TRACKING_CONFIDENCE = 0.5

    # Multi-person tracking settings
    MAX_PERSONS = 3
    PERSON_SIMILARITY_THRESHOLD = 0.7
    TRACKING_HISTORY_SIZE = 10

    # Temporal smoothing settings
    SMOOTHING_WINDOW_SIZE = 5
    SMOOTHING_SIGMA = 1.0

    # 3D pose estimation settings
    ENABLE_3D_ESTIMATION = True
    DEPTH_ESTIMATION_METHOD = "mediapipe"  # "mediapipe", "stereo", "monocular"

    # Visualization settings
    DRAW_CONNECTIONS = True
    DRAW_LANDMARKS = True
    LANDMARK_RADIUS = 3
    CONNECTION_THICKNESS = 2

# Pose landmark connections for MediaPipe
POSE_CONNECTIONS = mp.solutions.pose.POSE_CONNECTIONS

# Custom pose connections for martial arts and shooting analysis
MARTIAL_ARTS_CONNECTIONS = [
    # Core stability connections
    (11, 12),  # Shoulders
    (11, 23),  # Left shoulder to left hip
    (12, 24),  # Right shoulder to right hip
    (23, 24),  # Hips

    # Arm connections for shooting
    (11, 13),  # Left shoulder to left elbow
    (12, 14),  # Right shoulder to right elbow
    (13, 15),  # Left elbow to left wrist
    (14, 16),  # Right elbow to right wrist

    # Leg connections for stance
    (23, 25),  # Left hip to left knee
    (24, 26),  # Right hip to right knee
    (25, 27),  # Left knee to left ankle
    (26, 28),  # Right knee to right ankle
]

SHOOTING_ANALYSIS_POINTS = {
    'stance': [23, 24, 25, 26, 27, 28],  # Hip and leg points
    'grip': [15, 16, 19, 20],  # Wrist and hand points
    'sight_alignment': [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10],  # Face points
    'trigger_control': [15, 16],  # Wrist points
    'follow_through': [11, 12, 13, 14, 15, 16]  # Arm points
}

print("✅ Configuration loaded successfully!")

# Advanced Multi-Person Pose Estimator Class
class MultiPersonPoseEstimator:
    def __init__(self, config=PoseConfig()):
        self.config = config
        self.mp_pose = mp.solutions.pose
        self.mp_drawing = mp.solutions.drawing_utils
        self.mp_drawing_styles = mp.solutions.drawing_styles

        # Initialize MediaPipe Pose
        self.pose = self.mp_pose.Pose(
            static_image_mode=False,
            model_complexity=config.MEDIAPIPE_MODEL_COMPLEXITY,
            smooth_landmarks=config.MEDIAPIPE_SMOOTH_LANDMARKS,
            enable_segmentation=config.MEDIAPIPE_ENABLE_SEGMENTATION,
            smooth_segmentation=config.MEDIAPIPE_SMOOTH_SEGMENTATION,
            min_detection_confidence=config.MEDIAPIPE_MIN_DETECTION_CONFIDENCE,
            min_tracking_confidence=config.MEDIAPIPE_MIN_TRACKING_CONFIDENCE
        )

        # Person tracking
        self.person_tracks = {}
        self.next_person_id = 0
        self.tracking_history = deque(maxlen=config.TRACKING_HISTORY_SIZE)

        # Temporal smoothing
        self.smoothing_buffer = {}

    def calculate_person_similarity(self, landmarks1, landmarks2):
        """Calculate similarity between two pose landmarks"""
        if not landmarks1 or not landmarks2:
            return 0.0

        # Extract key points for comparison
        key_points = [0, 11, 12, 13, 14, 15, 16, 23, 24, 25, 26, 27, 28]

        points1 = []
        points2 = []

        for i in key_points:
            if i < len(landmarks1) and i < len(landmarks2):
                if landmarks1[i].visibility > 0.5 and landmarks2[i].visibility > 0.5:
                    points1.append([landmarks1[i].x, landmarks1[i].y])
                    points2.append([landmarks2[i].x, landmarks2[i].y])

        if len(points1) < 3 or len(points2) < 3:
            return 0.0

        # Calculate average distance
        points1 = np.array(points1)
        points2 = np.array(points2)

        # Normalize by image size
        distances = np.linalg.norm(points1 - points2, axis=1)
        avg_distance = np.mean(distances)

        # Convert distance to similarity (0-1)
        similarity = max(0, 1 - avg_distance * 2)
        return similarity

    def assign_person_ids(self, detected_poses):
        """Assign consistent IDs to detected poses"""
        current_poses = []

        for pose in detected_poses:
            if pose is None:
                continue

            best_match_id = None
            best_similarity = 0

            # Find best matching existing person
            for person_id, track in self.person_tracks.items():
                if len(track['landmarks_history']) > 0:
                    last_landmarks = track['landmarks_history'][-1]
                    similarity = self.calculate_person_similarity(pose.landmark, last_landmarks)

                    if similarity > best_similarity and similarity > self.config.PERSON_SIMILARITY_THRESHOLD:
                        best_similarity = similarity
                        best_match_id = person_id

            # Assign ID
            if best_match_id is not None:
                person_id = best_match_id
            else:
                person_id = self.next_person_id
                self.next_person_id += 1

            current_poses.append({
                'id': person_id,
                'landmarks': pose.landmark,
                'world_landmarks': pose.world_landmark if hasattr(pose, 'world_landmark') else None,
                'similarity': best_similarity
            })

        return current_poses

    def update_tracking_history(self, poses):
        """Update tracking history for all persons"""
        # Update existing tracks
        for pose in poses:
            person_id = pose['id']

            if person_id not in self.person_tracks:
                self.person_tracks[person_id] = {
                    'landmarks_history': deque(maxlen=self.config.TRACKING_HISTORY_SIZE),
                    'world_landmarks_history': deque(maxlen=self.config.TRACKING_HISTORY_SIZE),
                    'last_seen': time.time(),
                    'total_detections': 0
                }

            # Add to history
            self.person_tracks[person_id]['landmarks_history'].append(pose['landmarks'])
            if pose['world_landmarks']:
                self.person_tracks[person_id]['world_landmarks_history'].append(pose['world_landmarks'])

            self.person_tracks[person_id]['last_seen'] = time.time()
            self.person_tracks[person_id]['total_detections'] += 1

        # Clean up old tracks
        current_time = time.time()
        to_remove = []
        for person_id, track in self.person_tracks.items():
            if current_time - track['last_seen'] > 5.0:  # Remove after 5 seconds
                to_remove.append(person_id)

        for person_id in to_remove:
            del self.person_tracks[person_id]

    def apply_temporal_smoothing(self, landmarks):
        """Apply temporal smoothing to landmarks"""
        if len(landmarks) == 0:
            return landmarks

        smoothed_landmarks = []

        for i, landmark in enumerate(landmarks):
            if i not in self.smoothing_buffer:
                self.smoothing_buffer[i] = deque(maxlen=self.config.SMOOTHING_WINDOW_SIZE)

            # Add current point
            self.smoothing_buffer[i].append([landmark.x, landmark.y, landmark.z])

            # Calculate smoothed position
            if len(self.smoothing_buffer[i]) > 1:
                points = np.array(list(self.smoothing_buffer[i]))

                # Apply Gaussian smoothing
                weights = np.exp(-0.5 * ((np.arange(len(points)) - len(points) + 1) / self.config.SMOOTHING_SIGMA) ** 2)
                weights = weights / np.sum(weights)

                smoothed_point = np.average(points, axis=0, weights=weights)

                # Create new landmark with smoothed position
                smoothed_landmark = type(landmark)()
                smoothed_landmark.x = smoothed_point[0]
                smoothed_landmark.y = smoothed_point[1]
                smoothed_landmark.z = smoothed_point[2]
                smoothed_landmark.visibility = landmark.visibility

                smoothed_landmarks.append(smoothed_landmark)
            else:
                smoothed_landmarks.append(landmark)

        return smoothed_landmarks

    def process_frame(self, image):
        """Process a single frame and return multi-person pose data"""
        # Convert BGR to RGB
        rgb_image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)

        # Process with MediaPipe
        results = self.pose.process(rgb_image)

        detected_poses = []
        if results.pose_landmarks:
            # For multi-person, we'll process each detection
            # Note: MediaPipe Pose processes one person at a time
            # For true multi-person, we'd need to use a different approach
            detected_poses.append(results.pose_landmarks)

        # Assign person IDs
        poses_with_ids = self.assign_person_ids(detected_poses)

        # Update tracking history
        self.update_tracking_history(poses_with_ids)

        # Apply temporal smoothing
        smoothed_poses = []
        for pose in poses_with_ids:
            smoothed_landmarks = self.apply_temporal_smoothing(pose['landmarks'])
            smoothed_poses.append({
                'id': pose['id'],
                'landmarks': smoothed_landmarks,
                'world_landmarks': pose['world_landmarks'],
                'similarity': pose['similarity']
            })

        return {
            'poses': smoothed_poses,
            'raw_results': results,
            'image_shape': image.shape
        }

print("✅ MultiPersonPoseEstimator class created successfully!")

# Enhanced Multi-Person Detection using YOLO + MediaPipe
class EnhancedMultiPersonPoseEstimator:
    def __init__(self, config=PoseConfig()):
        self.config = config
        self.mp_pose = mp.solutions.pose
        self.mp_drawing = mp.solutions.drawing_utils

        # Initialize MediaPipe Pose
        self.pose = self.mp_pose.Pose(
            static_image_mode=False,
            model_complexity=config.MEDIAPIPE_MODEL_COMPLEXITY,
            smooth_landmarks=config.MEDIAPIPE_SMOOTH_LANDMARKS,
            enable_segmentation=config.MEDIAPIPE_ENABLE_SEGMENTATION,
            smooth_segmentation=config.MEDIAPIPE_SMOOTH_SEGMENTATION,
            min_detection_confidence=config.MEDIAPIPE_MIN_DETECTION_CONFIDENCE,
            min_tracking_confidence=config.MEDIAPIPE_MIN_TRACKING_CONFIDENCE
        )

        # Load YOLO for person detection
        try:
            from ultralytics import YOLO
            self.yolo_model = YOLO('yolov8n.pt')  # Nano version for speed
            self.use_yolo = True
            print("✅ YOLO model loaded successfully")
        except Exception as e:
            print(f"⚠️ YOLO not available, using fallback method: {e}")
            self.use_yolo = False

        # Person tracking
        self.person_tracks = {}
        self.next_person_id = 0
        self.smoothing_buffer = {}

    def detect_persons_yolo(self, image):
        """Detect persons using YOLO"""
        if not self.use_yolo:
            return []

        results = self.yolo_model(image, verbose=False)
        person_boxes = []

        for result in results:
            boxes = result.boxes
            if boxes is not None:
                for box in boxes:
                    # Check if it's a person (class 0 in COCO)
                    if int(box.cls[0]) == 0 and float(box.conf[0]) > 0.5:
                        x1, y1, x2, y2 = box.xyxy[0].cpu().numpy()
                        person_boxes.append({
                            'bbox': [int(x1), int(y1), int(x2), int(y2)],
                            'confidence': float(box.conf[0])
                        })

        return person_boxes

    def extract_person_roi(self, image, bbox):
        """Extract region of interest for a person"""
        x1, y1, x2, y2 = bbox
        # Add some padding
        padding = 20
        h, w = image.shape[:2]

        x1 = max(0, x1 - padding)
        y1 = max(0, y1 - padding)
        x2 = min(w, x2 + padding)
        y2 = min(h, y2 + padding)

        return image[y1:y2, x1:x2], (x1, y1, x2, y2)

    def process_person_roi(self, roi_image):
        """Process pose estimation for a single person ROI"""
        rgb_roi = cv2.cvtColor(roi_image, cv2.COLOR_BGR2RGB)
        results = self.pose.process(rgb_roi)

        if results.pose_landmarks:
            return results.pose_landmarks
        return None

    def transform_landmarks_to_full_image(self, landmarks, roi_coords, full_image_shape):
        """Transform landmarks from ROI coordinates to full image coordinates"""
        if not landmarks:
            return None

        x1, y1, x2, y2 = roi_coords
        roi_h, roi_w = y2 - y1, x2 - x1
        full_h, full_w = full_image_shape[:2]

        transformed_landmarks = []
        for landmark in landmarks.landmark:
            # Transform coordinates
            new_x = (landmark.x * roi_w + x1) / full_w
            new_y = (landmark.y * roi_h + y1) / full_h
            new_z = landmark.z  # Z coordinate doesn't need transformation

            # Create new landmark
            new_landmark = type(landmark)()
            new_landmark.x = new_x
            new_landmark.y = new_y
            new_landmark.z = new_z
            new_landmark.visibility = landmark.visibility

            transformed_landmarks.append(new_landmark)

        # Create new landmarks object
        new_landmarks = type(landmarks)()
        new_landmarks.landmark = transformed_landmarks
        new_landmarks.world_landmark = landmarks.world_landmark if hasattr(landmarks, 'world_landmark') else None

        return new_landmarks

    def calculate_person_similarity(self, landmarks1, landmarks2):
        """Calculate similarity between two pose landmarks"""
        if not landmarks1 or not landmarks2:
            return 0.0

        # Extract key points for comparison
        key_points = [0, 11, 12, 13, 14, 15, 16, 23, 24, 25, 26, 27, 28]

        points1 = []
        points2 = []

        for i in key_points:
            if i < len(landmarks1.landmark) and i < len(landmarks2.landmark):
                if landmarks1.landmark[i].visibility > 0.5 and landmarks2.landmark[i].visibility > 0.5:
                    points1.append([landmarks1.landmark[i].x, landmarks1.landmark[i].y])
                    points2.append([landmarks2.landmark[i].x, landmarks2.landmark[i].y])

        if len(points1) < 3 or len(points2) < 3:
            return 0.0

        # Calculate average distance
        points1 = np.array(points1)
        points2 = np.array(points2)

        distances = np.linalg.norm(points1 - points2, axis=1)
        avg_distance = np.mean(distances)

        # Convert distance to similarity (0-1)
        similarity = max(0, 1 - avg_distance * 2)
        return similarity

    def assign_person_ids(self, detected_poses):
        """Assign consistent IDs to detected poses"""
        current_poses = []

        for pose_data in detected_poses:
            if pose_data['landmarks'] is None:
                continue

            best_match_id = None
            best_similarity = 0

            # Find best matching existing person
            for person_id, track in self.person_tracks.items():
                if len(track['landmarks_history']) > 0:
                    last_landmarks = track['landmarks_history'][-1]
                    similarity = self.calculate_person_similarity(pose_data['landmarks'], last_landmarks)

                    if similarity > best_similarity and similarity > self.config.PERSON_SIMILARITY_THRESHOLD:
                        best_similarity = similarity
                        best_match_id = person_id

            # Assign ID
            if best_match_id is not None:
                person_id = best_match_id
            else:
                person_id = self.next_person_id
                self.next_person_id += 1

            current_poses.append({
                'id': person_id,
                'landmarks': pose_data['landmarks'],
                'world_landmarks': pose_data.get('world_landmarks'),
                'bbox': pose_data.get('bbox'),
                'similarity': best_similarity
            })

        return current_poses

    def update_tracking_history(self, poses):
        """Update tracking history for all persons"""
        current_time = time.time()

        for pose in poses:
            person_id = pose['id']

            if person_id not in self.person_tracks:
                self.person_tracks[person_id] = {
                    'landmarks_history': deque(maxlen=self.config.TRACKING_HISTORY_SIZE),
                    'world_landmarks_history': deque(maxlen=self.config.TRACKING_HISTORY_SIZE),
                    'bbox_history': deque(maxlen=self.config.TRACKING_HISTORY_SIZE),
                    'last_seen': current_time,
                    'total_detections': 0
                }

            # Add to history
            self.person_tracks[person_id]['landmarks_history'].append(pose['landmarks'])
            if pose.get('world_landmarks'):
                self.person_tracks[person_id]['world_landmarks_history'].append(pose['world_landmarks'])
            if pose.get('bbox'):
                self.person_tracks[person_id]['bbox_history'].append(pose['bbox'])

            self.person_tracks[person_id]['last_seen'] = current_time
            self.person_tracks[person_id]['total_detections'] += 1

        # Clean up old tracks
        to_remove = []
        for person_id, track in self.person_tracks.items():
            if current_time - track['last_seen'] > 5.0:  # Remove after 5 seconds
                to_remove.append(person_id)

        for person_id in to_remove:
            del self.person_tracks[person_id]

    def process_frame(self, image):
        """Process a single frame and return multi-person pose data"""
        detected_poses = []

        if self.use_yolo:
            # Use YOLO for person detection
            person_boxes = self.detect_persons_yolo(image)

            for person_box in person_boxes:
                bbox = person_box['bbox']
                roi_image, roi_coords = self.extract_person_roi(image, bbox)

                # Process pose for this person
                landmarks = self.process_person_roi(roi_image)

                if landmarks:
                    # Transform landmarks back to full image coordinates
                    full_landmarks = self.transform_landmarks_to_full_image(
                        landmarks, roi_coords, image.shape
                    )

                    detected_poses.append({
                        'landmarks': full_landmarks,
                        'world_landmarks': landmarks.world_landmark if hasattr(landmarks, 'world_landmark') else None,
                        'bbox': bbox
                    })
        else:
            # Fallback to single person detection
            rgb_image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)
            results = self.pose.process(rgb_image)

            if results.pose_landmarks:
                detected_poses.append({
                    'landmarks': results.pose_landmarks,
                    'world_landmarks': results.pose_world_landmarks if hasattr(results, 'pose_world_landmarks') else None,
                    'bbox': None
                })

        # Assign person IDs
        poses_with_ids = self.assign_person_ids(detected_poses)

        # Update tracking history
        self.update_tracking_history(poses_with_ids)

        return {
            'poses': poses_with_ids,
            'image_shape': image.shape,
            'num_persons_detected': len(poses_with_ids)
        }

print("✅ EnhancedMultiPersonPoseEstimator class created successfully!")

# 3D Pose Estimation and Analysis
class Pose3DAnalyzer:
    def __init__(self):
        self.landmark_names = [
            'nose', 'left_eye_inner', 'left_eye', 'left_eye_outer',
            'right_eye_inner', 'right_eye', 'right_eye_outer', 'left_ear',
            'right_ear', 'mouth_left', 'mouth_right', 'left_shoulder',
            'right_shoulder', 'left_elbow', 'right_elbow', 'left_wrist',
            'right_wrist', 'left_pinky', 'right_pinky', 'left_index',
            'right_index', 'left_thumb', 'right_thumb', 'left_hip',
            'right_hip', 'left_knee', 'right_knee', 'left_ankle', 'right_ankle',
            'left_heel', 'right_heel', 'left_foot_index', 'right_foot_index'
        ]

    def extract_3d_coordinates(self, landmarks):
        """Extract 3D coordinates from MediaPipe landmarks"""
        if not landmarks:
            return None

        coords_3d = []
        for landmark in landmarks.landmark:
            coords_3d.append({
                'x': landmark.x,
                'y': landmark.y,
                'z': landmark.z,
                'visibility': landmark.visibility
            })
        return coords_3d

    def calculate_angles_3d(self, landmarks):
        """Calculate 3D angles between key body parts"""
        if not landmarks or len(landmarks.landmark) < 33:
            return {}

        angles = {}

        # Helper function to get 3D vector between two points
        def get_vector3d(p1, p2):
            return np.array([p2.x - p1.x, p2.y - p1.y, p2.z - p1.z])

        # Helper function to calculate angle between two 3D vectors
        def angle_between_vectors(v1, v2):
            cos_angle = np.dot(v1, v2) / (np.linalg.norm(v1) * np.linalg.norm(v2))
            cos_angle = np.clip(cos_angle, -1.0, 1.0)
            return np.degrees(np.arccos(cos_angle))

        lm = landmarks.landmark

        # Elbow angles
        if (lm[11].visibility > 0.5 and lm[13].visibility > 0.5 and lm[15].visibility > 0.5):
            left_arm_vec1 = get_vector3d(lm[11], lm[13])  # shoulder to elbow
            left_arm_vec2 = get_vector3d(lm[13], lm[15])  # elbow to wrist
            angles['left_elbow'] = angle_between_vectors(left_arm_vec1, left_arm_vec2)

        if (lm[12].visibility > 0.5 and lm[14].visibility > 0.5 and lm[16].visibility > 0.5):
            right_arm_vec1 = get_vector3d(lm[12], lm[14])  # shoulder to elbow
            right_arm_vec2 = get_vector3d(lm[14], lm[16])  # elbow to wrist
            angles['right_elbow'] = angle_between_vectors(right_arm_vec1, right_arm_vec2)

        # Knee angles
        if (lm[23].visibility > 0.5 and lm[25].visibility > 0.5 and lm[27].visibility > 0.5):
            left_leg_vec1 = get_vector3d(lm[23], lm[25])  # hip to knee
            left_leg_vec2 = get_vector3d(lm[25], lm[27])  # knee to ankle
            angles['left_knee'] = angle_between_vectors(left_leg_vec1, left_leg_vec2)

        if (lm[24].visibility > 0.5 and lm[26].visibility > 0.5 and lm[28].visibility > 0.5):
            right_leg_vec1 = get_vector3d(lm[24], lm[26])  # hip to knee
            right_leg_vec2 = get_vector3d(lm[26], lm[28])  # knee to ankle
            angles['right_knee'] = angle_between_vectors(right_leg_vec1, right_leg_vec2)

        # Shoulder angles (for shooting stance analysis)
        if (lm[11].visibility > 0.5 and lm[12].visibility > 0.5 and lm[23].visibility > 0.5 and lm[24].visibility > 0.5):
            shoulder_vec = get_vector3d(lm[11], lm[12])  # left to right shoulder
            hip_vec = get_vector3d(lm[23], lm[24])       # left to right hip
            angles['shoulder_hip_alignment'] = angle_between_vectors(shoulder_vec, hip_vec)

        return angles

    def analyze_shooting_stance(self, landmarks):
        """Analyze shooting stance quality"""
        if not landmarks or len(landmarks.landmark) < 33:
            return {}

        analysis = {}
        lm = landmarks.landmark

        # Stance width analysis
        if lm[23].visibility > 0.5 and lm[24].visibility > 0.5:
            hip_width = abs(lm[24].x - lm[23].x)
            analysis['stance_width'] = hip_width
            analysis['stance_width_rating'] = 'good' if 0.1 < hip_width < 0.3 else 'needs_adjustment'

        # Shoulder alignment
        if (lm[11].visibility > 0.5 and lm[12].visibility > 0.5 and
            lm[23].visibility > 0.5 and lm[24].visibility > 0.5):
            shoulder_center = (lm[11].x + lm[12].x) / 2
            hip_center = (lm[23].x + lm[24].x) / 2
            alignment_offset = abs(shoulder_center - hip_center)
            analysis['shoulder_hip_alignment'] = alignment_offset
            analysis['alignment_rating'] = 'good' if alignment_offset < 0.05 else 'needs_adjustment'

        # Arm extension for shooting
        if (lm[12].visibility > 0.5 and lm[14].visibility > 0.5 and lm[16].visibility > 0.5):
            # Calculate arm extension (shoulder to wrist distance)
            arm_length = np.sqrt(
                (lm[16].x - lm[12].x)**2 +
                (lm[16].y - lm[12].y)**2 +
                (lm[16].z - lm[12].z)**2
            )
            analysis['arm_extension'] = arm_length
            analysis['extension_rating'] = 'good' if arm_length > 0.3 else 'needs_extension'

        # Grip analysis (wrist to hand points)
        if (lm[16].visibility > 0.5 and lm[20].visibility > 0.5):
            grip_stability = abs(lm[20].x - lm[16].x) + abs(lm[20].y - lm[16].y)
            analysis['grip_stability'] = grip_stability
            analysis['grip_rating'] = 'stable' if grip_stability < 0.1 else 'unstable'

        return analysis

    def analyze_martial_arts_stance(self, landmarks):
        """Analyze martial arts stance quality"""
        if not landmarks or len(landmarks.landmark) < 33:
            return {}

        analysis = {}
        lm = landmarks.landmark

        # Center of gravity analysis
        if (lm[23].visibility > 0.5 and lm[24].visibility > 0.5 and
            lm[25].visibility > 0.5 and lm[26].visibility > 0.5):
            # Calculate center of gravity based on hip and knee positions
            cog_x = (lm[23].x + lm[24].x + lm[25].x + lm[26].x) / 4
            cog_y = (lm[23].y + lm[24].y + lm[25].y + lm[26].y) / 4
            analysis['center_of_gravity'] = {'x': cog_x, 'y': cog_y}

            # Check if COG is centered between feet
            if lm[27].visibility > 0.5 and lm[28].visibility > 0.5:
                foot_center_x = (lm[27].x + lm[28].x) / 2
                cog_offset = abs(cog_x - foot_center_x)
                analysis['cog_balance'] = cog_offset
                analysis['balance_rating'] = 'balanced' if cog_offset < 0.1 else 'unbalanced'

        # Stance depth (front to back)
        if lm[25].visibility > 0.5 and lm[26].visibility > 0.5:
            stance_depth = abs(lm[26].y - lm[25].y)
            analysis['stance_depth'] = stance_depth
            analysis['depth_rating'] = 'good' if 0.05 < stance_depth < 0.2 else 'needs_adjustment'

        # Knee bend analysis
        angles = self.calculate_angles_3d(landmarks)
        if 'left_knee' in angles and 'right_knee' in angles:
            knee_bend_avg = (angles['left_knee'] + angles['right_knee']) / 2
            analysis['knee_bend'] = knee_bend_avg
            analysis['knee_rating'] = 'good' if 120 < knee_bend_avg < 160 else 'needs_adjustment'

        return analysis

    def detect_hidden_body_parts(self, landmarks, previous_landmarks=None):
        """Detect and estimate hidden body parts using temporal analysis"""
        if not landmarks or len(landmarks.landmark) < 33:
            return {}

        hidden_parts = {}
        lm = landmarks.landmark

        # Analyze visibility of key points
        key_points = {
            'left_shoulder': 11, 'right_shoulder': 12,
            'left_elbow': 13, 'right_elbow': 14,
            'left_wrist': 15, 'right_wrist': 16,
            'left_hip': 23, 'right_hip': 24,
            'left_knee': 25, 'right_knee': 26,
            'left_ankle': 27, 'right_ankle': 28
        }

        for part_name, idx in key_points.items():
            if lm[idx].visibility < 0.5:  # Part is hidden or poorly visible
                hidden_parts[part_name] = {
                    'visibility': lm[idx].visibility,
                    'estimated_position': {
                        'x': lm[idx].x,
                        'y': lm[idx].y,
                        'z': lm[idx].z
                    },
                    'confidence': 'low'
                }

                # If we have previous landmarks, try to estimate position
                if previous_landmarks and len(previous_landmarks.landmark) > idx:
                    prev_lm = previous_landmarks.landmark[idx]
                    if prev_lm.visibility > 0.5:
                        # Use previous position as estimate
                        hidden_parts[part_name]['estimated_position'] = {
                            'x': prev_lm.x,
                            'y': prev_lm.y,
                            'z': prev_lm.z
                        }
                        hidden_parts[part_name]['confidence'] = 'medium'

        return hidden_parts

print("✅ Pose3DAnalyzer class created successfully!")

# Visualization and Analysis Tools
class PoseVisualizer:
    def __init__(self):
        self.colors = [
            (255, 0, 0),    # Red
            (0, 255, 0),    # Green
            (0, 0, 255),    # Blue
            (255, 255, 0),  # Yellow
            (255, 0, 255),  # Magenta
            (0, 255, 255),  # Cyan
        ]

    def draw_pose_2d(self, image, poses, draw_connections=True, draw_landmarks=True):
        """Draw 2D pose visualization on image"""
        annotated_image = image.copy()

        for i, pose_data in enumerate(poses):
            person_id = pose_data['id']
            landmarks = pose_data['landmarks']
            bbox = pose_data.get('bbox')

            # Choose color for this person
            color = self.colors[person_id % len(self.colors)]

            # Draw bounding box if available
            if bbox:
                x1, y1, x2, y2 = bbox
                cv2.rectangle(annotated_image, (x1, y1), (x2, y2), color, 2)
                cv2.putText(annotated_image, f"Person {person_id}", (x1, y1-10),
                           cv2.FONT_HERSHEY_SIMPLEX, 0.7, color, 2)

            if landmarks:
                h, w = image.shape[:2]

                # Draw landmarks
                if draw_landmarks:
                    for landmark in landmarks.landmark:
                        if landmark.visibility > 0.5:
                            x = int(landmark.x * w)
                            y = int(landmark.y * h)
                            cv2.circle(annotated_image, (x, y), 3, color, -1)

                # Draw connections
                if draw_connections:
                    for connection in MARTIAL_ARTS_CONNECTIONS:
                        start_idx, end_idx = connection
                        if (start_idx < len(landmarks.landmark) and
                            end_idx < len(landmarks.landmark)):

                            start_lm = landmarks.landmark[start_idx]
                            end_lm = landmarks.landmark[end_idx]

                            if start_lm.visibility > 0.5 and end_lm.visibility > 0.5:
                                start_x = int(start_lm.x * w)
                                start_y = int(start_lm.y * h)
                                end_x = int(end_lm.x * w)
                                end_y = int(end_lm.y * h)

                                cv2.line(annotated_image, (start_x, start_y), (end_x, end_y), color, 2)

        return annotated_image

    def create_3d_plot(self, poses, title="3D Pose Visualization"):
        """Create 3D plot of pose landmarks"""
        fig = go.Figure()

        for i, pose_data in enumerate(poses):
            person_id = pose_data['id']
            landmarks = pose_data['landmarks']

            if landmarks:
                # Extract 3D coordinates
                x_coords = [lm.x for lm in landmarks.landmark]
                y_coords = [lm.y for lm in landmarks.landmark]
                z_coords = [lm.z for lm in landmarks.landmark]
                visibility = [lm.visibility for lm in landmarks.landmark]

                # Filter visible points
                visible_x = [x for x, v in zip(x_coords, visibility) if v > 0.5]
                visible_y = [y for y, v in zip(y_coords, visibility) if v > 0.5]
                visible_z = [z for z, v in zip(z_coords, visibility) if v > 0.5]

                # Add scatter plot for landmarks
                fig.add_trace(go.Scatter3d(
                    x=visible_x,
                    y=visible_y,
                    z=visible_z,
                    mode='markers',
                    marker=dict(
                        size=5,
                        color=self.colors[person_id % len(self.colors)],
                        opacity=0.8
                    ),
                    name=f'Person {person_id}',
                    text=[f'Landmark {j}' for j in range(len(visible_x))],
                    hovertemplate='<b>%{text}</b><br>X: %{x:.3f}<br>Y: %{y:.3f}<br>Z: %{z:.3f}<extra></extra>'
                ))

                # Add connections
                for connection in MARTIAL_ARTS_CONNECTIONS:
                    start_idx, end_idx = connection
                    if (start_idx < len(landmarks.landmark) and
                        end_idx < len(landmarks.landmark)):

                        start_lm = landmarks.landmark[start_idx]
                        end_lm = landmarks.landmark[end_idx]

                        if start_lm.visibility > 0.5 and end_lm.visibility > 0.5:
                            fig.add_trace(go.Scatter3d(
                                x=[start_lm.x, end_lm.x],
                                y=[start_lm.y, end_lm.y],
                                z=[start_lm.z, end_lm.z],
                                mode='lines',
                                line=dict(
                                    color=self.colors[person_id % len(self.colors)],
                                    width=3
                                ),
                                showlegend=False,
                                hoverinfo='skip'
                            ))

        fig.update_layout(
            title=title,
            scene=dict(
                xaxis_title='X',
                yaxis_title='Y',
                zaxis_title='Z',
                aspectmode='data'
            ),
            width=800,
            height=600
        )

        return fig

    def create_analysis_dashboard(self, poses, analysis_data):
        """Create analysis dashboard with multiple plots"""
        if not poses:
            return None

        # Create subplots
        fig = make_subplots(
            rows=2, cols=2,
            subplot_titles=('Pose Analysis', '3D Visualization', 'Angles', 'Stance Quality'),
            specs=[[{"type": "scatter"}, {"type": "scatter3d"}],
                   [{"type": "bar"}, {"type": "bar"}]]
        )

        # Add pose analysis (placeholder for now)
        fig.add_trace(
            go.Scatter(x=[0, 1], y=[0, 1], mode='markers', name='Pose Points'),
            row=1, col=1
        )

        # Add 3D visualization
        for i, pose_data in enumerate(poses):
            landmarks = pose_data['landmarks']
            if landmarks:
                x_coords = [lm.x for lm in landmarks.landmark if lm.visibility > 0.5]
                y_coords = [lm.y for lm in landmarks.landmark if lm.visibility > 0.5]
                z_coords = [lm.z for lm in landmarks.landmark if lm.visibility > 0.5]

                fig.add_trace(
                    go.Scatter3d(
                        x=x_coords, y=y_coords, z=z_coords,
                        mode='markers',
                        name=f'Person {pose_data["id"]}',
                        marker=dict(size=3)
                    ),
                    row=1, col=2
                )

        # Add angles analysis
        if 'angles' in analysis_data:
            angles = analysis_data['angles']
            angle_names = list(angles.keys())
            angle_values = list(angles.values())

            fig.add_trace(
                go.Bar(x=angle_names, y=angle_values, name='Joint Angles'),
                row=2, col=1
            )

        # Add stance quality
        if 'stance_analysis' in analysis_data:
            stance = analysis_data['stance_analysis']
            metrics = list(stance.keys())
            values = [stance[m] if isinstance(stance[m], (int, float)) else 0 for m in metrics]

            fig.add_trace(
                go.Bar(x=metrics, y=values, name='Stance Metrics'),
                row=2, col=2
            )

        fig.update_layout(
            title="Pose Analysis Dashboard",
            height=800,
            showlegend=True
        )

        return fig

print("✅ PoseVisualizer class created successfully!")

# Video Processing and Testing
class VideoProcessor:
    def __init__(self, pose_estimator, pose_analyzer, visualizer):
        self.pose_estimator = pose_estimator
        self.pose_analyzer = pose_analyzer
        self.visualizer = visualizer
        self.frame_count = 0
        self.results_history = []

    def process_video_file(self, video_path, output_path=None, max_frames=None):
        """Process a video file and return results"""
        cap = cv2.VideoCapture(video_path)

        if not cap.isOpened():
            print(f"Error: Could not open video file {video_path}")
            return None

        fps = int(cap.get(cv2.CAP_PROP_FPS))
        total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))
        width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))
        height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))

        print(f"Video Info: {width}x{height}, {fps} FPS, {total_frames} frames")

        # Setup video writer if output path is provided
        out = None
        if output_path:
            fourcc = cv2.VideoWriter_fourcc(*'mp4v')
            out = cv2.VideoWriter(output_path, fourcc, fps, (width, height))

        frame_results = []
        frame_count = 0

        while True:
            ret, frame = cap.read()
            if not ret:
                break

            if max_frames and frame_count >= max_frames:
                break

            # Process frame
            start_time = time.time()
            result = self.pose_estimator.process_frame(frame)
            processing_time = time.time() - start_time

            # Analyze poses
            analysis_results = []
            for pose_data in result['poses']:
                landmarks = pose_data['landmarks']
                if landmarks:
                    # 3D analysis
                    coords_3d = self.pose_analyzer.extract_3d_coordinates(landmarks)
                    angles = self.pose_analyzer.calculate_angles_3d(landmarks)
                    shooting_analysis = self.pose_analyzer.analyze_shooting_stance(landmarks)
                    martial_arts_analysis = self.pose_analyzer.analyze_martial_arts_stance(landmarks)
                    hidden_parts = self.pose_analyzer.detect_hidden_body_parts(landmarks)

                    analysis_results.append({
                        'person_id': pose_data['id'],
                        'coords_3d': coords_3d,
                        'angles': angles,
                        'shooting_analysis': shooting_analysis,
                        'martial_arts_analysis': martial_arts_analysis,
                        'hidden_parts': hidden_parts
                    })

            # Draw visualization
            annotated_frame = self.visualizer.draw_pose_2d(frame, result['poses'])

            # Add processing info
            cv2.putText(annotated_frame, f"Frame: {frame_count}", (10, 30),
                       cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 255, 255), 2)
            cv2.putText(annotated_frame, f"Persons: {len(result['poses'])}", (10, 70),
                       cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 255, 255), 2)
            cv2.putText(annotated_frame, f"Time: {processing_time:.3f}s", (10, 110),
                       cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 255, 255), 2)

            # Write frame if output is specified
            if out:
                out.write(annotated_frame)

            # Store results
            frame_results.append({
                'frame_number': frame_count,
                'poses': result['poses'],
                'analysis': analysis_results,
                'processing_time': processing_time,
                'timestamp': frame_count / fps
            })

            frame_count += 1

            # Print progress
            if frame_count % 30 == 0:
                print(f"Processed {frame_count}/{total_frames} frames")

        cap.release()
        if out:
            out.release()

        print(f"Video processing complete. Processed {frame_count} frames.")
        return frame_results

    def process_webcam(self, duration=10):
        """Process webcam feed for real-time testing"""
        cap = cv2.VideoCapture(0)

        if not cap.isOpened():
            print("Error: Could not open webcam")
            return

        start_time = time.time()
        frame_count = 0

        print("Starting webcam processing. Press 'q' to quit.")

        while True:
            ret, frame = cap.read()
            if not ret:
                break

            # Check duration
            if time.time() - start_time > duration:
                break

            # Process frame
            result = self.pose_estimator.process_frame(frame)

            # Draw visualization
            annotated_frame = self.visualizer.draw_pose_2d(frame, result['poses'])

            # Add info
            cv2.putText(annotated_frame, f"Frame: {frame_count}", (10, 30),
                       cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 255, 255), 2)
            cv2.putText(annotated_frame, f"Persons: {len(result['poses'])}", (10, 70),
                       cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 255, 255), 2)

            # Show frame
            cv2.imshow('Pose Estimation', annotated_frame)

            # Break on 'q' key
            if cv2.waitKey(1) & 0xFF == ord('q'):
                break

            frame_count += 1

        cap.release()
        cv2.destroyAllWindows()
        print(f"Webcam processing complete. Processed {frame_count} frames.")

    def create_sample_video(self, output_path="sample_poses.mp4", duration=5):
        """Create a sample video with pose estimation for testing"""
        # Create a simple test video with moving rectangles (simulating people)
        fourcc = cv2.VideoWriter_fourcc(*'mp4v')
        out = cv2.VideoWriter(output_path, fourcc, 30, (640, 480))

        for frame_num in range(duration * 30):
            # Create a black frame
            frame = np.zeros((480, 640, 3), dtype=np.uint8)

            # Add some moving rectangles to simulate people
            x1 = int(100 + 50 * np.sin(frame_num * 0.1))
            y1 = int(200 + 30 * np.cos(frame_num * 0.1))
            x2 = int(400 + 40 * np.sin(frame_num * 0.15))
            y2 = int(150 + 25 * np.cos(frame_num * 0.12))

            cv2.rectangle(frame, (x1, y1), (x1+100, y1+200), (255, 255, 255), -1)
            cv2.rectangle(frame, (x2, y2), (x2+100, y2+200), (255, 255, 255), -1)

            out.write(frame)

        out.release()
        print(f"Sample video created: {output_path}")

print("✅ VideoProcessor class created successfully!")

# Initialize the complete system
print("🚀 Initializing Multi-Person 3D Pose Estimation System...")

# Create configuration
config = PoseConfig()

# Initialize components
pose_estimator = EnhancedMultiPersonPoseEstimator(config)
pose_analyzer = Pose3DAnalyzer()
visualizer = PoseVisualizer()
video_processor = VideoProcessor(pose_estimator, pose_analyzer, visualizer)

print("✅ System initialized successfully!")
print(f"📊 Configuration:")
print(f"   - Max persons: {config.MAX_PERSONS}")
print(f"   - Model complexity: {config.MEDIAPIPE_MODEL_COMPLEXITY}")
print(f"   - 3D estimation: {config.ENABLE_3D_ESTIMATION}")
print(f"   - YOLO enabled: {pose_estimator.use_yolo}")
print(f"   - Smoothing window: {config.SMOOTHING_WINDOW_SIZE}")

# Test with a sample image
print("\n🧪 Testing with sample image...")

# Create a test image
test_image = np.zeros((480, 640, 3), dtype=np.uint8)
cv2.rectangle(test_image, (100, 100), (300, 400), (255, 255, 255), -1)
cv2.rectangle(test_image, (400, 150), (600, 450), (255, 255, 255), -1)

# Process the test image
result = pose_estimator.process_frame(test_image)
print(f"✅ Test completed. Detected {len(result['poses'])} persons.")

# Display system status
print(f"\n📈 System Status:")
print(f"   - Pose estimator: ✅ Ready")
print(f"   - 3D analyzer: ✅ Ready")
print(f"   - Visualizer: ✅ Ready")
print(f"   - Video processor: ✅ Ready")
print(f"   - Person tracks: {len(pose_estimator.person_tracks)}")

# Test with a single image
import cv2
image=cv2.imread("/content/test.jpg")
result=pose_estimator.process_frame(image)
annotated=visualizer.draw_pose_2d(image,result['poses'])
cv2.imshow("Result",annotated)

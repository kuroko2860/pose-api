{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b364689a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages for Google Colab\n",
    "# Reinstalling mediapipe, tensorflow, and scipy with compatible versions to resolve numpy conflict\n",
    "!pip install mediapipe==0.10.21 tensorflow==2.19.0 scipy==1.13.1 opencv-python numpy matplotlib scikit-learn\n",
    "!pip install tensorflow-hub\n",
    "!pip install torch torchvision\n",
    "!pip install ultralytics\n",
    "!pip install supervision\n",
    "\n",
    "# For OpenPose (alternative approach)\n",
    "!pip install opencv-contrib-python\n",
    "\n",
    "# Additional packages for 3D visualization\n",
    "!pip install plotly\n",
    "!pip install trimesh\n",
    "\n",
    "print(\"✅ All packages installed successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97ccf6ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import all necessary libraries\n",
    "import cv2\n",
    "import numpy as np\n",
    "import mediapipe as mp\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patches\n",
    "from matplotlib.animation import FuncAnimation\n",
    "import plotly.graph_objects as go\n",
    "import plotly.express as px\n",
    "from plotly.subplots import make_subplots\n",
    "import json\n",
    "import time\n",
    "from collections import deque\n",
    "from scipy import signal\n",
    "from scipy.spatial.distance import cdist\n",
    "from sklearn.cluster import DBSCAN\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# TensorFlow and TensorFlow Hub for additional models\n",
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "\n",
    "# PyTorch for advanced models\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "print(\"✅ All libraries imported successfully!\")\n",
    "print(f\"OpenCV version: {cv2.__version__}\")\n",
    "print(f\"MediaPipe version: {mp.__version__}\")\n",
    "print(f\"TensorFlow version: {tf.__version__}\")\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abd5629b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pose estimation configuration and constants\n",
    "class PoseConfig:\n",
    "    # MediaPipe settings\n",
    "    MEDIAPIPE_MODEL_COMPLEXITY = 2  # 0, 1, or 2 (higher = more accurate)\n",
    "    MEDIAPIPE_SMOOTH_LANDMARKS = True\n",
    "    MEDIAPIPE_ENABLE_SEGMENTATION = False\n",
    "    MEDIAPIPE_SMOOTH_SEGMENTATION = True\n",
    "    MEDIAPIPE_MIN_DETECTION_CONFIDENCE = 0.5\n",
    "    MEDIAPIPE_MIN_TRACKING_CONFIDENCE = 0.5\n",
    "\n",
    "    # Multi-person tracking settings\n",
    "    MAX_PERSONS = 3\n",
    "    PERSON_SIMILARITY_THRESHOLD = 0.7\n",
    "    TRACKING_HISTORY_SIZE = 10\n",
    "\n",
    "    # Temporal smoothing settings\n",
    "    SMOOTHING_WINDOW_SIZE = 5\n",
    "    SMOOTHING_SIGMA = 1.0\n",
    "\n",
    "    # 3D pose estimation settings\n",
    "    ENABLE_3D_ESTIMATION = True\n",
    "    DEPTH_ESTIMATION_METHOD = \"mediapipe\"  # \"mediapipe\", \"stereo\", \"monocular\"\n",
    "\n",
    "    # Visualization settings\n",
    "    DRAW_CONNECTIONS = True\n",
    "    DRAW_LANDMARKS = True\n",
    "    LANDMARK_RADIUS = 3\n",
    "    CONNECTION_THICKNESS = 2\n",
    "\n",
    "# Pose landmark connections for MediaPipe\n",
    "POSE_CONNECTIONS = mp.solutions.pose.POSE_CONNECTIONS\n",
    "\n",
    "# Custom pose connections for martial arts and shooting analysis\n",
    "MARTIAL_ARTS_CONNECTIONS = [\n",
    "    # Core stability connections\n",
    "    (11, 12),  # Shoulders\n",
    "    (11, 23),  # Left shoulder to left hip\n",
    "    (12, 24),  # Right shoulder to right hip\n",
    "    (23, 24),  # Hips\n",
    "\n",
    "    # Arm connections for shooting\n",
    "    (11, 13),  # Left shoulder to left elbow\n",
    "    (12, 14),  # Right shoulder to right elbow\n",
    "    (13, 15),  # Left elbow to left wrist\n",
    "    (14, 16),  # Right elbow to right wrist\n",
    "\n",
    "    # Leg connections for stance\n",
    "    (23, 25),  # Left hip to left knee\n",
    "    (24, 26),  # Right hip to right knee\n",
    "    (25, 27),  # Left knee to left ankle\n",
    "    (26, 28),  # Right ankle to right ankle\n",
    "]\n",
    "\n",
    "SHOOTING_ANALYSIS_POINTS = {\n",
    "    'stance': [23, 24, 25, 26, 27, 28],  # Hip and leg points\n",
    "    'grip': [15, 16, 19, 20],  # Wrist and hand points\n",
    "    'sight_alignment': [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10],  # Face points\n",
    "    'trigger_control': [15, 16],  # Wrist points\n",
    "    'follow_through': [11, 12, 13, 14, 15, 16]  # Arm points\n",
    "}\n",
    "\n",
    "print(\"✅ Configuration loaded successfully!\")\n",
    "\n",
    "# Advanced Multi-Person Pose Estimator Class\n",
    "class MultiPersonPoseEstimator:\n",
    "    def __init__(self, config=PoseConfig()):\n",
    "        self.config = config\n",
    "        self.mp_pose = mp.solutions.pose\n",
    "        self.mp_drawing = mp.solutions.drawing_utils\n",
    "        self.mp_drawing_styles = mp.solutions.drawing_styles\n",
    "\n",
    "        # Initialize MediaPipe Pose\n",
    "        self.pose = self.mp_pose.Pose(\n",
    "            static_image_mode=False,\n",
    "            model_complexity=config.MEDIAPIPE_MODEL_COMPLEXITY,\n",
    "            smooth_landmarks=config.MEDIAPIPE_SMOOTH_LANDMARKS,\n",
    "            enable_segmentation=config.MEDIAPIPE_ENABLE_SEGMENTATION,\n",
    "            smooth_segmentation=config.MEDIAPIPE_SMOOTH_SEGMENTATION,\n",
    "            min_detection_confidence=config.MEDIAPIPE_MIN_DETECTION_CONFIDENCE,\n",
    "            min_tracking_confidence=config.MEDIAPIPE_MIN_TRACKING_CONFIDENCE\n",
    "        )\n",
    "\n",
    "        # Person tracking\n",
    "        self.person_tracks = {}\n",
    "        self.next_person_id = 0\n",
    "        self.tracking_history = deque(maxlen=config.TRACKING_HISTORY_SIZE)\n",
    "\n",
    "        # Temporal smoothing\n",
    "        self.smoothing_buffer = {}\n",
    "\n",
    "    def calculate_person_similarity(self, landmarks1, landmarks2):\n",
    "        \"\"\"Calculate similarity between two pose landmarks\"\"\"\n",
    "        if not landmarks1 or not landmarks2:\n",
    "            return 0.0\n",
    "\n",
    "        # Extract key points for comparison\n",
    "        key_points = [0, 11, 12, 13, 14, 15, 16, 23, 24, 25, 26, 27, 28]\n",
    "\n",
    "        points1 = []\n",
    "        points2 = []\n",
    "\n",
    "        for i in key_points:\n",
    "            if i < len(landmarks1) and i < len(landmarks2):\n",
    "                if landmarks1[i].visibility > 0.5 and landmarks2[i].visibility > 0.5:\n",
    "                    points1.append([landmarks1[i].x, landmarks1[i].y])\n",
    "                    points2.append([landmarks2[i].x, landmarks2[i].y])\n",
    "\n",
    "        if len(points1) < 3 or len(points2) < 3:\n",
    "            return 0.0\n",
    "\n",
    "        # Calculate average distance\n",
    "        points1 = np.array(points1)\n",
    "        points2 = np.array(points2)\n",
    "\n",
    "        # Normalize by image size\n",
    "        distances = np.linalg.norm(points1 - points2, axis=1)\n",
    "        avg_distance = np.mean(distances)\n",
    "\n",
    "        # Convert distance to similarity (0-1)\n",
    "        similarity = max(0, 1 - avg_distance * 2)\n",
    "        return similarity\n",
    "\n",
    "    def assign_person_ids(self, detected_poses):\n",
    "        \"\"\"Assign consistent IDs to detected poses\"\"\"\n",
    "        current_poses = []\n",
    "\n",
    "        for pose in detected_poses:\n",
    "            if pose is None:\n",
    "                continue\n",
    "\n",
    "            best_match_id = None\n",
    "            best_similarity = 0\n",
    "\n",
    "            # Find best matching existing person\n",
    "            for person_id, track in self.person_tracks.items():\n",
    "                if len(track['landmarks_history']) > 0:\n",
    "                    last_landmarks = track['landmarks_history'][-1]\n",
    "                    similarity = self.calculate_person_similarity(pose.landmark, last_landmarks)\n",
    "\n",
    "                    if similarity > best_similarity and similarity > self.config.PERSON_SIMILARITY_THRESHOLD:\n",
    "                        best_similarity = similarity\n",
    "                        best_match_id = person_id\n",
    "\n",
    "            # Assign ID\n",
    "            if best_match_id is not None:\n",
    "                person_id = best_match_id\n",
    "            else:\n",
    "                person_id = self.next_person_id\n",
    "                self.next_person_id += 1\n",
    "\n",
    "            current_poses.append({\n",
    "                'id': person_id,\n",
    "                'landmarks': pose.landmark,\n",
    "                'world_landmarks': pose.world_landmark if hasattr(pose, 'world_landmark') else None,\n",
    "                'similarity': best_similarity\n",
    "            })\n",
    "\n",
    "        return current_poses\n",
    "\n",
    "    def update_tracking_history(self, poses):\n",
    "        \"\"\"Update tracking history for all persons\"\"\"\n",
    "        # Update existing tracks\n",
    "        for pose in poses:\n",
    "            person_id = pose['id']\n",
    "\n",
    "            if person_id not in self.person_tracks:\n",
    "                self.person_tracks[person_id] = {\n",
    "                    'landmarks_history': deque(maxlen=self.config.TRACKING_HISTORY_SIZE),\n",
    "                    'world_landmarks_history': deque(maxlen=self.config.TRACKING_HISTORY_SIZE),\n",
    "                    'last_seen': time.time(),\n",
    "                    'total_detections': 0\n",
    "                }\n",
    "\n",
    "            # Add to history\n",
    "            self.person_tracks[person_id]['landmarks_history'].append(pose['landmarks'])\n",
    "            if pose['world_landmarks']:\n",
    "                self.person_tracks[person_id]['world_landmarks_history'].append(pose['world_landmarks'])\n",
    "\n",
    "            self.person_tracks[person_id]['last_seen'] = time.time()\n",
    "            self.person_tracks[person_id]['total_detections'] += 1\n",
    "\n",
    "        # Clean up old tracks\n",
    "        current_time = time.time()\n",
    "        to_remove = []\n",
    "        for person_id, track in self.person_tracks.items():\n",
    "            if current_time - track['last_seen'] > 5.0:  # Remove after 5 seconds\n",
    "                to_remove.append(person_id)\n",
    "\n",
    "        for person_id in to_remove:\n",
    "            del self.person_tracks[person_id]\n",
    "\n",
    "    def apply_temporal_smoothing(self, landmarks):\n",
    "        \"\"\"Apply temporal smoothing to landmarks\"\"\"\n",
    "        if len(landmarks) == 0:\n",
    "            return landmarks\n",
    "\n",
    "        smoothed_landmarks = []\n",
    "\n",
    "        for i, landmark in enumerate(landmarks):\n",
    "            if i not in self.smoothing_buffer:\n",
    "                self.smoothing_buffer[i] = deque(maxlen=self.config.SMOOTHING_WINDOW_SIZE)\n",
    "\n",
    "            # Add current point\n",
    "            self.smoothing_buffer[i].append([landmark.x, landmark.y, landmark.z])\n",
    "\n",
    "            # Calculate smoothed position\n",
    "            if len(self.smoothing_buffer[i]) > 1:\n",
    "                points = np.array(list(self.smoothing_buffer[i]))\n",
    "\n",
    "                # Apply Gaussian smoothing\n",
    "                weights = np.exp(-0.5 * ((np.arange(len(points)) - len(points) + 1) / self.config.SMOOTHING_SIGMA) ** 2)\n",
    "                weights = weights / np.sum(weights)\n",
    "\n",
    "                smoothed_point = np.average(points, axis=0, weights=weights)\n",
    "\n",
    "                # Create new landmark with smoothed position\n",
    "                smoothed_landmark = type(landmark)()\n",
    "                smoothed_landmark.x = smoothed_point[0]\n",
    "                smoothed_landmark.y = smoothed_point[1]\n",
    "                smoothed_landmark.z = smoothed_point[2]\n",
    "                smoothed_landmark.visibility = landmark.visibility\n",
    "\n",
    "                smoothed_landmarks.append(smoothed_landmark)\n",
    "            else:\n",
    "                smoothed_landmarks.append(landmark)\n",
    "\n",
    "        return smoothed_landmarks\n",
    "\n",
    "    def process_frame(self, image):\n",
    "        \"\"\"Process a single frame and return multi-person pose data\"\"\"\n",
    "        # Convert BGR to RGB\n",
    "        rgb_image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "        # Process with MediaPipe\n",
    "        results = self.pose.process(rgb_image)\n",
    "\n",
    "        detected_poses = []\n",
    "        if results.pose_landmarks:\n",
    "            # For multi-person, we'll process each detection\n",
    "            # Note: MediaPipe Pose processes one person at a time\n",
    "            # For true multi-person, we'd need to use a different approach\n",
    "            detected_poses.append(results.pose_landmarks)\n",
    "\n",
    "        # Assign person IDs\n",
    "        poses_with_ids = self.assign_person_ids(detected_poses)\n",
    "\n",
    "        # Update tracking history\n",
    "        self.update_tracking_history(poses_with_ids)\n",
    "\n",
    "        # Apply temporal smoothing\n",
    "        smoothed_poses = []\n",
    "        for pose in poses_with_ids:\n",
    "            smoothed_landmarks = self.apply_temporal_smoothing(pose['landmarks'])\n",
    "            smoothed_poses.append({\n",
    "                'id': pose['id'],\n",
    "                'landmarks': smoothed_landmarks,\n",
    "                'world_landmarks': pose['world_landmarks'],\n",
    "                'similarity': pose['similarity']\n",
    "            })\n",
    "\n",
    "        return {\n",
    "            'poses': smoothed_poses,\n",
    "            'raw_results': results,\n",
    "            'image_shape': image.shape\n",
    "        }\n",
    "\n",
    "print(\"✅ MultiPersonPoseEstimator class created successfully!\")\n",
    "\n",
    "# Advanced Multi-Person Detection with Improved Tracking\n",
    "class AdvancedMultiPersonPoseEstimator:\n",
    "    def __init__(self, config=PoseConfig()):\n",
    "        self.config = config\n",
    "        self.mp_pose = mp.solutions.pose\n",
    "        self.mp_drawing = mp.solutions.drawing_utils\n",
    "\n",
    "        # Initialize MediaPipe Pose with higher accuracy settings\n",
    "        # Disable segmentation smoothing to prevent dimension mismatch errors\n",
    "        self.pose = self.mp_pose.Pose(\n",
    "            static_image_mode=False,\n",
    "            model_complexity=2,  # Maximum complexity for better accuracy\n",
    "            smooth_landmarks=True,\n",
    "            enable_segmentation=False,  # Disable segmentation to prevent smoothing errors\n",
    "            smooth_segmentation=False,  # Disable segmentation smoothing\n",
    "            min_detection_confidence=0.6,  # Higher confidence threshold\n",
    "            min_tracking_confidence=0.6\n",
    "        )\n",
    "\n",
    "        # Load YOLO for person detection\n",
    "        try:\n",
    "            from ultralytics import YOLO\n",
    "            # Use YOLOv8m for better accuracy with multiple people\n",
    "            self.yolo_model = YOLO('yolov8m.pt')  # Medium version for better accuracy\n",
    "            self.use_yolo = True\n",
    "            print(\"✅ YOLO model loaded successfully\")\n",
    "        except Exception as e:\n",
    "            print(f\"⚠️ YOLO not available, using fallback method: {e}\")\n",
    "            self.use_yolo = False\n",
    "\n",
    "        # Advanced person tracking\n",
    "        self.person_tracks = {}\n",
    "        self.next_person_id = 0\n",
    "        self.smoothing_buffer = {}\n",
    "        self.tracking_history = deque(maxlen=30)  # Longer history for better tracking\n",
    "\n",
    "        # Kalman filter for each person (simple implementation)\n",
    "        self.kalman_filters = {}\n",
    "\n",
    "        # Overlap detection\n",
    "        self.overlap_threshold = 0.3  # IoU threshold for overlap detection\n",
    "\n",
    "        # Performance optimization\n",
    "        self.frame_skip = 2  # Process every 2nd frame for YOLO detection\n",
    "        self.frame_count = 0\n",
    "\n",
    "    def detect_persons_yolo(self, image):\n",
    "        \"\"\"Detect persons using YOLO with improved filtering\"\"\"\n",
    "        if not self.use_yolo:\n",
    "            return []\n",
    "\n",
    "        # Only run YOLO detection every few frames for performance\n",
    "        if self.frame_count % self.frame_skip != 0:\n",
    "            return self.get_cached_detections()\n",
    "\n",
    "        results = self.yolo_model(image, verbose=False)\n",
    "        person_boxes = []\n",
    "\n",
    "        for result in results:\n",
    "            boxes = result.boxes\n",
    "            if boxes is not None:\n",
    "                for box in boxes:\n",
    "                    # Check if it's a person (class 0 in COCO) with higher confidence\n",
    "                    if int(box.cls[0]) == 0 and float(box.conf[0]) > 0.6:  # Higher confidence threshold\n",
    "                        x1, y1, x2, y2 = box.xyxy[0].cpu().numpy()\n",
    "\n",
    "                        # Filter out very small detections\n",
    "                        width = x2 - x1\n",
    "                        height = y2 - y1\n",
    "                        if width > 50 and height > 100:  # Minimum size filter\n",
    "                            person_boxes.append({\n",
    "                                'bbox': [int(x1), int(y1), int(x2), int(y2)],\n",
    "                                'confidence': float(box.conf[0]),\n",
    "                                'area': width * height\n",
    "                            })\n",
    "\n",
    "        # Sort by confidence and area\n",
    "        person_boxes.sort(key=lambda x: (x['confidence'], x['area']), reverse=True)\n",
    "\n",
    "        # Cache detections for frame skipping\n",
    "        self.cached_detections = person_boxes\n",
    "        return person_boxes\n",
    "\n",
    "    def get_cached_detections(self):\n",
    "        \"\"\"Get cached detections when skipping YOLO processing\"\"\"\n",
    "        return getattr(self, 'cached_detections', [])\n",
    "\n",
    "    def calculate_iou(self, box1, box2):\n",
    "        \"\"\"Calculate Intersection over Union (IoU) of two bounding boxes\"\"\"\n",
    "        x1_1, y1_1, x2_1, y2_1 = box1\n",
    "        x1_2, y1_2, x2_2, y2_2 = box2\n",
    "\n",
    "        # Calculate intersection\n",
    "        x1_i = max(x1_1, x1_2)\n",
    "        y1_i = max(y1_1, y1_2)\n",
    "        x2_i = min(x2_1, x2_2)\n",
    "        y2_i = min(y2_1, y2_2)\n",
    "\n",
    "        if x2_i <= x1_i or y2_i <= y1_i:\n",
    "            return 0.0\n",
    "\n",
    "        intersection = (x2_i - x1_i) * (y2_i - y1_i)\n",
    "        area1 = (x2_1 - x1_1) * (y2_1 - y1_1)\n",
    "        area2 = (x2_2 - x1_2) * (y2_2 - y1_2)\n",
    "        union = area1 + area2 - intersection\n",
    "\n",
    "        return intersection / union if union > 0 else 0.0\n",
    "\n",
    "    def detect_overlaps(self, person_boxes):\n",
    "        \"\"\"Detect overlapping persons and handle them\"\"\"\n",
    "        overlaps = []\n",
    "        processed = set()\n",
    "\n",
    "        for i, box1_data in enumerate(person_boxes):\n",
    "            if i in processed:\n",
    "                continue\n",
    "\n",
    "            box1 = box1_data['bbox']\n",
    "\n",
    "            overlap_group = [i]\n",
    "            for j, box2_data in enumerate(person_boxes[i+1:], i+1):\n",
    "                if j in processed:\n",
    "                    continue\n",
    "\n",
    "                box2 = box2_data['bbox']\n",
    "\n",
    "                iou = self.calculate_iou(box1, box2)\n",
    "                if iou > self.overlap_threshold:\n",
    "                    overlap_group.append(j)\n",
    "                    processed.add(j)\n",
    "\n",
    "            if len(overlap_group) > 1:\n",
    "                overlaps.append(overlap_group)\n",
    "                processed.update(overlap_group)\n",
    "\n",
    "        return overlaps\n",
    "\n",
    "    def extract_person_roi(self, image, bbox, padding=20):\n",
    "        \"\"\"Extract region of interest for a person with configurable padding using letterbox scaling\"\"\"\n",
    "        x1, y1, x2, y2 = bbox\n",
    "        h, w = image.shape[:2]\n",
    "\n",
    "        x1 = max(0, x1 - padding)\n",
    "        y1 = max(0, y1 - padding)\n",
    "        x2 = min(w, x2 + padding)\n",
    "        y2 = min(h, y2 + padding)\n",
    "\n",
    "        # Extract ROI\n",
    "        roi_image = image[y1:y2, x1:x2]\n",
    "        original_roi_h, original_roi_w = roi_image.shape[:2]\n",
    "\n",
    "        # Target dimensions for consistent processing\n",
    "        target_height = 480\n",
    "        target_width = 320\n",
    "\n",
    "        # Calculate scale factor to fit ROI into target dimensions while preserving aspect ratio\n",
    "        scale_x = target_width / original_roi_w\n",
    "        scale_y = target_height / original_roi_h\n",
    "        scale = min(scale_x, scale_y)  # Use smaller scale to fit entirely\n",
    "\n",
    "        # Calculate new dimensions after scaling\n",
    "        new_w = int(original_roi_w * scale)\n",
    "        new_h = int(original_roi_h * scale)\n",
    "\n",
    "        # Resize ROI with preserved aspect ratio\n",
    "        if new_w != original_roi_w or new_h != original_roi_h:\n",
    "            roi_image = cv2.resize(roi_image, (new_w, new_h))\n",
    "\n",
    "        # Create letterbox (add padding to reach target dimensions)\n",
    "        letterbox_image = np.zeros((target_height, target_width, 3), dtype=np.uint8)\n",
    "\n",
    "        # Calculate padding offsets to center the image\n",
    "        pad_x = (target_width - new_w) // 2\n",
    "        pad_y = (target_height - new_h) // 2\n",
    "\n",
    "        # Place the scaled ROI in the center of the letterbox\n",
    "        letterbox_image[pad_y:pad_y + new_h, pad_x:pad_x + new_w] = roi_image\n",
    "\n",
    "        # Store transformation parameters for proper landmark transformation\n",
    "        roi_coords = (x1, y1, x2, y2, original_roi_w, original_roi_h, scale, pad_x, pad_y)\n",
    "\n",
    "        return letterbox_image, roi_coords\n",
    "\n",
    "    def apply_advanced_smoothing(self, poses):\n",
    "        \"\"\"Apply advanced temporal smoothing to poses\"\"\"\n",
    "        smoothed_poses = []\n",
    "\n",
    "        for pose_data in poses:\n",
    "            person_id = pose_data['id']\n",
    "            landmarks = pose_data['landmarks']\n",
    "\n",
    "            if landmarks:\n",
    "                # Apply smoothing to landmarks\n",
    "                smoothed_landmarks = self.smooth_landmarks_temporally(person_id, landmarks)\n",
    "\n",
    "                # Apply smoothing to bounding box\n",
    "                smoothed_bbox = self.smooth_bbox_temporally(person_id, pose_data.get('bbox'))\n",
    "\n",
    "                smoothed_poses.append({\n",
    "                    'id': person_id,\n",
    "                    'landmarks': smoothed_landmarks,\n",
    "                    'world_landmarks': pose_data.get('world_landmarks'),\n",
    "                    'bbox': smoothed_bbox,\n",
    "                    'confidence': pose_data.get('confidence', 1.0),\n",
    "                    'in_overlap': pose_data.get('in_overlap', False),\n",
    "                    'similarity': pose_data.get('similarity', 0.0)\n",
    "                })\n",
    "\n",
    "        return smoothed_poses\n",
    "\n",
    "    def smooth_landmarks_temporally(self, person_id, landmarks):\n",
    "        \"\"\"Apply temporal smoothing to landmarks using exponential moving average\"\"\"\n",
    "        if person_id not in self.smoothing_buffer:\n",
    "            self.smoothing_buffer[person_id] = {\n",
    "                'landmarks_history': deque(maxlen=10),\n",
    "                'alpha': 0.7  # Smoothing factor\n",
    "            }\n",
    "\n",
    "        buffer = self.smoothing_buffer[person_id]\n",
    "        buffer['landmarks_history'].append(landmarks)\n",
    "\n",
    "        if len(buffer['landmarks_history']) == 1:\n",
    "            return landmarks\n",
    "\n",
    "        # Apply exponential moving average\n",
    "        alpha = buffer['alpha']\n",
    "        smoothed_landmarks_list = []\n",
    "\n",
    "        for i, landmark in enumerate(landmarks.landmark):\n",
    "            # Get previous smoothed landmark from history\n",
    "            if len(buffer['landmarks_history']) > 1:\n",
    "                prev_landmarks = buffer['landmarks_history'][-2]\n",
    "                if i < len(prev_landmarks.landmark):\n",
    "                    prev_landmark = prev_landmarks.landmark[i]\n",
    "                    if hasattr(prev_landmark, 'x'):\n",
    "                        prev_x, prev_y, prev_z = prev_landmark.x, prev_landmark.y, prev_landmark.z\n",
    "                    else:\n",
    "                        prev_x = prev_landmark.get('x', 0)\n",
    "                        prev_y = prev_landmark.get('y', 0)\n",
    "                        prev_z = prev_landmark.get('z', 0)\n",
    "                else:\n",
    "                    prev_x = prev_y = prev_z = 0\n",
    "            else:\n",
    "                prev_x = prev_y = prev_z = 0\n",
    "\n",
    "\n",
    "            # Handle both MediaPipe and simple landmarks\n",
    "            if hasattr(landmark, 'x'):\n",
    "                new_x = alpha * landmark.x + (1 - alpha) * prev_x\n",
    "                new_y = alpha * landmark.y + (1 - alpha) * prev_y\n",
    "                new_z = alpha * landmark.z + (1 - alpha) * prev_z\n",
    "                new_visibility = landmark.visibility\n",
    "            else:\n",
    "                new_x = alpha * landmark.get('x', 0) + (1 - alpha) * prev_x\n",
    "                new_y = alpha * landmark.get('y', 0) + (1 - alpha) * prev_y\n",
    "                new_z = alpha * landmark.get('z', 0) + (1 - alpha) * prev_z\n",
    "                new_visibility = landmark.get('visibility', 0)\n",
    "\n",
    "\n",
    "            # Create smoothed landmark dictionary\n",
    "            smoothed_landmark_dict = {\n",
    "                'x': new_x,\n",
    "                'y': new_y,\n",
    "                'z': new_z,\n",
    "                'visibility': new_visibility\n",
    "            }\n",
    "            smoothed_landmarks_list.append(smoothed_landmark_dict)\n",
    "\n",
    "        # Create new landmarks object (MediaPipe or simple)\n",
    "        if hasattr(landmarks, 'landmark'):\n",
    "            from mediapipe.framework.formats import landmark_pb2\n",
    "            new_landmarks = landmark_pb2.NormalizedLandmarkList()\n",
    "            for smoothed_landmark_dict in smoothed_landmarks_list:\n",
    "                new_landmark = new_landmarks.landmark.add()\n",
    "                new_landmark.x = smoothed_landmark_dict['x']\n",
    "                new_landmark.y = smoothed_landmark_dict['y']\n",
    "                new_landmark.z = smoothed_landmark_dict['z']\n",
    "                new_landmark.visibility = smoothed_landmark_dict['visibility']\n",
    "\n",
    "            # Copy world landmarks if available\n",
    "            if hasattr(landmarks, 'world_landmark') and landmarks.world_landmark:\n",
    "                 for world_lm in landmarks.world_landmark:\n",
    "                      new_world_landmark = new_landmarks.world_landmark.add()\n",
    "                      new_world_landmark.x = world_lm.x\n",
    "                      new_world_landmark.y = world_lm.y\n",
    "                      new_world_landmark.z = world_lm.z\n",
    "                      new_world_landmark.visibility = world_lm.visibility\n",
    "\n",
    "\n",
    "            return new_landmarks\n",
    "        else:\n",
    "            # Simple landmarks object\n",
    "            class SimpleLandmarks:\n",
    "                def __init__(self, landmark_list):\n",
    "                    self.landmark = landmark_list\n",
    "                    self.world_landmark = None\n",
    "            return SimpleLandmarks(smoothed_landmarks_list)\n",
    "\n",
    "    def smooth_bbox_temporally(self, person_id, bbox):\n",
    "        \"\"\"Apply temporal smoothing to bounding box\"\"\"\n",
    "        if not bbox:\n",
    "            return bbox\n",
    "\n",
    "        if person_id not in self.smoothing_buffer:\n",
    "            self.smoothing_buffer[person_id] = {\n",
    "                'bbox_history': deque(maxlen=5),\n",
    "                'alpha': 0.8\n",
    "            }\n",
    "\n",
    "        buffer = self.smoothing_buffer[person_id]\n",
    "        if 'bbox_history' not in buffer:\n",
    "            buffer['bbox_history'] = deque(maxlen=5)\n",
    "\n",
    "        buffer['bbox_history'].append(bbox)\n",
    "\n",
    "        if len(buffer['bbox_history']) == 1:\n",
    "            return bbox\n",
    "\n",
    "        # Apply exponential moving average to bbox\n",
    "        alpha = buffer['alpha']\n",
    "        prev_bbox = buffer['bbox_history'][-2]\n",
    "\n",
    "        smoothed_bbox = [\n",
    "            alpha * bbox[0] + (1 - alpha) * prev_bbox[0],\n",
    "            alpha * bbox[1] + (1 - alpha) * prev_bbox[1],\n",
    "            alpha * bbox[2] + (1 - alpha) * prev_bbox[2],\n",
    "            alpha * bbox[3] + (1 - alpha) * prev_bbox[3]\n",
    "        ]\n",
    "\n",
    "        return smoothed_bbox\n",
    "\n",
    "    def process_person_roi(self, roi_image):\n",
    "        \"\"\"Process pose estimation for a single person ROI\"\"\"\n",
    "        rgb_roi = cv2.cvtColor(roi_image, cv2.COLOR_BGR2RGB)\n",
    "        results = self.pose.process(rgb_roi)\n",
    "\n",
    "        if results.pose_landmarks:\n",
    "            return results.pose_landmarks\n",
    "        return None\n",
    "\n",
    "    def transform_landmarks_to_full_image(self, landmarks, roi_coords, full_image_shape):\n",
    "        \"\"\"Transform landmarks from ROI coordinates to full image coordinates with letterbox support\"\"\"\n",
    "        if not landmarks:\n",
    "            return None\n",
    "\n",
    "        full_h, full_w = full_image_shape[:2]\n",
    "        target_height = 480\n",
    "        target_width = 320\n",
    "\n",
    "        # Handle different ROI coordinate formats\n",
    "        if len(roi_coords) == 9:  # New format with letterbox parameters\n",
    "            x1, y1, x2, y2, original_roi_w, original_roi_h, scale, pad_x, pad_y = roi_coords\n",
    "        elif len(roi_coords) == 6:  # Old format without letterbox\n",
    "            x1, y1, x2, y2, original_roi_w, original_roi_h = roi_coords\n",
    "            scale = 1.0\n",
    "            pad_x = pad_y = 0\n",
    "        else:  # Very old format\n",
    "            x1, y1, x2, y2 = roi_coords\n",
    "            original_roi_w, original_roi_h = x2 - x1, y2 - y1\n",
    "            scale = 1.0\n",
    "            pad_x = pad_y = 0\n",
    "\n",
    "        try:\n",
    "            # Try using MediaPipe's proper method\n",
    "            from mediapipe.framework.formats import landmark_pb2\n",
    "\n",
    "            new_landmarks = landmark_pb2.NormalizedLandmarkList()\n",
    "\n",
    "            for i, landmark in enumerate(landmarks.landmark):\n",
    "                # Transform coordinates from letterbox ROI back to original ROI, then to full image\n",
    "\n",
    "                # Step 1: Remove letterbox padding (convert from letterbox coordinates to scaled ROI coordinates)\n",
    "                letterbox_x = landmark.x * target_width\n",
    "                letterbox_y = landmark.y * target_height\n",
    "\n",
    "                # Remove padding offset\n",
    "                scaled_x = letterbox_x - pad_x\n",
    "                scaled_y = letterbox_y - pad_y\n",
    "\n",
    "                # Step 2: Convert from scaled ROI coordinates back to original ROI coordinates\n",
    "                original_roi_x = scaled_x / scale\n",
    "                original_roi_y = scaled_y / scale\n",
    "\n",
    "                # Step 3: Transform to full image coordinates\n",
    "                new_x = (original_roi_x + x1) / full_w\n",
    "                new_y = (original_roi_y + y1) / full_h\n",
    "                new_z = landmark.z  # Z coordinate doesn't need transformation\n",
    "\n",
    "                # Add new landmark to the list\n",
    "                new_landmark = new_landmarks.landmark.add()\n",
    "                new_landmark.x = new_x\n",
    "                new_landmark.y = new_y\n",
    "                new_landmark.z = new_z\n",
    "                new_landmark.visibility = landmark.visibility\n",
    "\n",
    "            # Copy world landmarks if available\n",
    "            if hasattr(landmarks, 'world_landmark') and landmarks.world_landmark:\n",
    "                for world_lm in landmarks.world_landmark:\n",
    "                    new_world_landmark = new_landmarks.world_landmark.add()\n",
    "                    new_world_landmark.x = world_lm.x\n",
    "                    new_world_landmark.y = world_lm.y\n",
    "                    new_world_landmark.z = world_lm.z\n",
    "                    new_world_landmark.visibility = world_lm.visibility\n",
    "\n",
    "            return new_landmarks\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"⚠️ MediaPipe landmark creation failed: {e}\")\n",
    "            # Fallback: return a simple dictionary representation\n",
    "            transformed_landmarks = []\n",
    "            for i, landmark in enumerate(landmarks.landmark):\n",
    "                # Transform coordinates from letterbox ROI back to original ROI, then to full image\n",
    "\n",
    "                # Step 1: Remove letterbox padding\n",
    "                letterbox_x = landmark.x * target_width\n",
    "                letterbox_y = landmark.y * target_height\n",
    "\n",
    "                # Remove padding offset\n",
    "                scaled_x = letterbox_x - pad_x\n",
    "                scaled_y = letterbox_y - pad_y\n",
    "\n",
    "                # Step 2: Convert from scaled ROI coordinates back to original ROI coordinates\n",
    "                original_roi_x = scaled_x / scale\n",
    "                original_roi_y = scaled_y / scale\n",
    "\n",
    "                # Step 3: Transform to full image coordinates\n",
    "                new_x = (original_roi_x + x1) / full_w\n",
    "                new_y = (original_roi_y + y1) / full_h\n",
    "                new_z = landmark.z\n",
    "\n",
    "                transformed_landmarks.append({\n",
    "                    'x': new_x,\n",
    "                    'y': new_y,\n",
    "                    'z': new_z,\n",
    "                    'visibility': landmark.visibility\n",
    "                })\n",
    "\n",
    "            # Create a simple object that mimics MediaPipe landmarks\n",
    "            class SimpleLandmarks:\n",
    "                def __init__(self, landmark_list):\n",
    "                    self.landmark = landmark_list\n",
    "                    self.world_landmark = None\n",
    "\n",
    "            return SimpleLandmarks(transformed_landmarks)\n",
    "\n",
    "    def calculate_person_similarity(self, landmarks1, landmarks2, bbox1=None, bbox2=None):\n",
    "        \"\"\"Calculate similarity between two pose landmarks with multiple features\"\"\"\n",
    "        if not landmarks1 or not landmarks2:\n",
    "            return 0.0\n",
    "\n",
    "        similarity_score = 0.0\n",
    "        weights = []\n",
    "\n",
    "        # 1. Pose landmark similarity (weight: 0.6)\n",
    "        pose_sim = self.calculate_pose_similarity(landmarks1, landmarks2)\n",
    "        similarity_score += pose_sim * 0.6\n",
    "        weights.append(0.6)\n",
    "\n",
    "        # 2. Bounding box similarity (weight: 0.3)\n",
    "        if bbox1 and bbox2:\n",
    "            bbox_sim = self.calculate_bbox_similarity(bbox1, bbox2)\n",
    "            similarity_score += bbox_sim * 0.3\n",
    "            weights.append(0.3)\n",
    "\n",
    "        # 3. Size similarity (weight: 0.1)\n",
    "        if bbox1 and bbox2:\n",
    "            size_sim = self.calculate_size_similarity(bbox1, bbox2)\n",
    "            similarity_score += size_sim * 0.1\n",
    "            weights.append(0.1)\n",
    "\n",
    "        # Normalize by total weight\n",
    "        total_weight = sum(weights)\n",
    "        return similarity_score / total_weight if total_weight > 0 else 0.0\n",
    "\n",
    "    def calculate_pose_similarity(self, landmarks1, landmarks2):\n",
    "        \"\"\"Calculate pose landmark similarity\"\"\"\n",
    "        # Extract key points for comparison\n",
    "        key_points = [0, 11, 12, 13, 14, 15, 16, 23, 24, 25, 26, 27, 28]\n",
    "\n",
    "        points1 = []\n",
    "        points2 = []\n",
    "\n",
    "        for i in key_points:\n",
    "            if i < len(landmarks1.landmark) and i < len(landmarks2.landmark):\n",
    "                # Handle both MediaPipe and simple landmarks\n",
    "                if hasattr(landmarks1.landmark[i], 'visibility'):\n",
    "                    vis1 = landmarks1.landmark[i].visibility\n",
    "                    x1 = landmarks1.landmark[i].x\n",
    "                    y1 = landmarks1.landmark[i].y\n",
    "                else:\n",
    "                    vis1 = landmarks1.landmark[i].get('visibility', 0)\n",
    "                    x1 = landmarks1.landmark[i].get('x', 0)\n",
    "                    y1 = landmarks1.landmark[i].get('y', 0)\n",
    "\n",
    "                if hasattr(landmarks2.landmark[i], 'visibility'):\n",
    "                    vis2 = landmarks2.landmark[i].visibility\n",
    "                    x2 = landmarks2.landmark[i].x\n",
    "                    y2 = landmarks2.landmark[i].y\n",
    "                else:\n",
    "                    vis2 = landmarks2.landmark[i].get('visibility', 0)\n",
    "                    x2 = landmarks2.landmark[i].get('x', 0)\n",
    "                    y2 = landmarks2.landmark[i].get('y', 0)\n",
    "\n",
    "                if vis1 > 0.5 and vis2 > 0.5:\n",
    "                    points1.append([x1, y1])\n",
    "                    points2.append([x2, y2])\n",
    "\n",
    "        if len(points1) < 3 or len(points2) < 3:\n",
    "            return 0.0\n",
    "\n",
    "        # Calculate average distance\n",
    "        points1 = np.array(points1)\n",
    "        points2 = np.array(points2)\n",
    "\n",
    "        distances = np.linalg.norm(points1 - points2, axis=1)\n",
    "        avg_distance = np.mean(distances)\n",
    "\n",
    "        # Convert distance to similarity (0-1)\n",
    "        similarity = max(0, 1 - avg_distance * 3)  # More sensitive to distance\n",
    "        return similarity\n",
    "\n",
    "    def calculate_bbox_similarity(self, bbox1, bbox2):\n",
    "        \"\"\"Calculate bounding box similarity using IoU\"\"\"\n",
    "        iou = self.calculate_iou(bbox1, bbox2)\n",
    "        return iou\n",
    "\n",
    "    def calculate_size_similarity(self, bbox1, bbox2):\n",
    "        \"\"\"Calculate size similarity between bounding boxes\"\"\"\n",
    "        area1 = (bbox1[2] - bbox1[0]) * (bbox1[3] - bbox1[1])\n",
    "        area2 = (bbox2[2] - bbox2[0]) * (bbox2[3] - bbox2[1])\n",
    "\n",
    "        if area1 == 0 or area2 == 0:\n",
    "            return 0.0\n",
    "\n",
    "        ratio = min(area1, area2) / max(area1, area2)\n",
    "        return ratio\n",
    "\n",
    "    def predict_next_position(self, person_id, current_bbox):\n",
    "        \"\"\"Predict next position using simple motion model\"\"\"\n",
    "        if person_id not in self.kalman_filters:\n",
    "            # Initialize simple motion model\n",
    "            self.kalman_filters[person_id] = {\n",
    "                'velocity': [0, 0],\n",
    "                'last_position': current_bbox[:2],\n",
    "                'last_time': time.time()\n",
    "            }\n",
    "            return current_bbox\n",
    "\n",
    "        # Simple velocity-based prediction\n",
    "        kalman = self.kalman_filters[person_id]\n",
    "        current_time = time.time()\n",
    "        dt = current_time - kalman['last_time']\n",
    "\n",
    "        if dt > 0:\n",
    "            # Update velocity\n",
    "            current_center = [(current_bbox[0] + current_bbox[2]) / 2,\n",
    "                            (current_bbox[1] + current_bbox[3]) / 2]\n",
    "            last_center = kalman['last_position']\n",
    "\n",
    "            velocity = [(current_center[0] - last_center[0]) / dt,\n",
    "                       (current_center[1] - last_center[1]) / dt]\n",
    "\n",
    "            # Smooth velocity\n",
    "            kalman['velocity'] = [0.7 * kalman['velocity'][0] + 0.3 * velocity[0],\n",
    "                                0.7 * kalman['velocity'][1] + 0.3 * velocity[1]]\n",
    "\n",
    "            # Predict next position\n",
    "            predicted_center = [current_center[0] + kalman['velocity'][0] * dt,\n",
    "                              current_center[1] + kalman['velocity'][1] * dt]\n",
    "\n",
    "            # Convert back to bbox\n",
    "            width = current_bbox[2] - current_bbox[0]\n",
    "            height = current_bbox[3] - current_bbox[1]\n",
    "\n",
    "            predicted_bbox = [\n",
    "                predicted_center[0] - width / 2,\n",
    "                predicted_center[1] - height / 2,\n",
    "                predicted_center[0] + width / 2,\n",
    "                predicted_center[1] + height / 2\n",
    "            ]\n",
    "\n",
    "            # Update state\n",
    "            kalman['last_position'] = current_center\n",
    "            kalman['last_time'] = current_time\n",
    "\n",
    "            return predicted_bbox\n",
    "\n",
    "        return current_bbox\n",
    "\n",
    "    def assign_person_ids(self, detected_poses):\n",
    "        \"\"\"Assign consistent IDs to detected poses using Hungarian algorithm\"\"\"\n",
    "        if not detected_poses:\n",
    "            return []\n",
    "\n",
    "        # Create cost matrix for Hungarian algorithm\n",
    "        existing_ids = list(self.person_tracks.keys())\n",
    "        new_poses = [pose for pose in detected_poses if pose['landmarks'] is not None]\n",
    "\n",
    "        if not new_poses:\n",
    "            return []\n",
    "\n",
    "        # Initialize cost matrix\n",
    "        cost_matrix = np.full((len(existing_ids), len(new_poses)), float('inf'))\n",
    "\n",
    "        # Calculate costs\n",
    "        for i, person_id in enumerate(existing_ids):\n",
    "            track = self.person_tracks[person_id]\n",
    "            if len(track['landmarks_history']) > 0:\n",
    "                last_landmarks = track['landmarks_history'][-1]\n",
    "                last_bbox = track['bbox_history'][-1] if track['bbox_history'] else None\n",
    "\n",
    "                for j, pose_data in enumerate(new_poses):\n",
    "                    # Calculate similarity (lower is better for Hungarian)\n",
    "                    similarity = self.calculate_person_similarity(\n",
    "                        pose_data['landmarks'],\n",
    "                        last_landmarks,\n",
    "                        pose_data.get('bbox'),\n",
    "                        last_bbox\n",
    "                    )\n",
    "\n",
    "                    # Convert similarity to cost (1 - similarity)\n",
    "                    cost = 1.0 - similarity\n",
    "                    cost_matrix[i, j] = cost\n",
    "\n",
    "        # Use Hungarian algorithm to find optimal assignment\n",
    "        try:\n",
    "            from scipy.optimize import linear_sum_assignment\n",
    "            row_indices, col_indices = linear_sum_assignment(cost_matrix)\n",
    "\n",
    "            # Create assignments\n",
    "            assignments = {}\n",
    "            for i, j in zip(row_indices, col_indices):\n",
    "                if cost_matrix[i, j] < 0.5:  # Only assign if cost is low enough\n",
    "                    person_id = existing_ids[i]\n",
    "                    assignments[j] = person_id\n",
    "        except ImportError:\n",
    "            # Fallback to greedy assignment\n",
    "            assignments = {}\n",
    "            used_poses = set()\n",
    "            for i, person_id in enumerate(existing_ids):\n",
    "                best_pose_idx = None\n",
    "                best_cost = float('inf')\n",
    "\n",
    "                for j, pose_data in enumerate(new_poses):\n",
    "                    if j in used_poses:\n",
    "                        continue\n",
    "\n",
    "                    if cost_matrix[i, j] < best_cost:\n",
    "                        best_cost = cost_matrix[i, j]\n",
    "                        best_pose_idx = j\n",
    "\n",
    "                if best_pose_idx is not None and best_cost < 0.5:\n",
    "                    assignments[best_pose_idx] = person_id\n",
    "                    used_poses.add(best_pose_idx)\n",
    "\n",
    "        # Assign IDs\n",
    "        current_poses = []\n",
    "        for j, pose_data in enumerate(new_poses):\n",
    "            if j in assignments:\n",
    "                person_id = assignments[j]\n",
    "                similarity = 1.0 - cost_matrix[existing_ids.index(person_id), j]\n",
    "            else:\n",
    "                # Create new person ID\n",
    "                person_id = self.next_person_id\n",
    "                self.next_person_id += 1\n",
    "                similarity = 0.0\n",
    "\n",
    "            current_poses.append({\n",
    "                'id': person_id,\n",
    "                'landmarks': pose_data['landmarks'],\n",
    "                'world_landmarks': pose_data.get('world_landmarks'),\n",
    "                'bbox': pose_data.get('bbox'),\n",
    "                'confidence': pose_data.get('confidence', 1.0),\n",
    "                'in_overlap': pose_data.get('in_overlap', False),\n",
    "                'similarity': similarity\n",
    "            })\n",
    "\n",
    "        return current_poses\n",
    "\n",
    "    def update_tracking_history(self, poses):\n",
    "        \"\"\"Update tracking history for all persons\"\"\"\n",
    "        current_time = time.time()\n",
    "\n",
    "        for pose in poses:\n",
    "            person_id = pose['id']\n",
    "\n",
    "            if person_id not in self.person_tracks:\n",
    "                self.person_tracks[person_id] = {\n",
    "                    'landmarks_history': deque(maxlen=self.config.TRACKING_HISTORY_SIZE),\n",
    "                    'world_landmarks_history': deque(maxlen=self.config.TRACKING_HISTORY_SIZE),\n",
    "                    'bbox_history': deque(maxlen=self.config.TRACKING_HISTORY_SIZE),\n",
    "                    'last_seen': current_time,\n",
    "                    'total_detections': 0\n",
    "                }\n",
    "\n",
    "            # Add to history\n",
    "            self.person_tracks[person_id]['landmarks_history'].append(pose['landmarks'])\n",
    "            if pose.get('world_landmarks'):\n",
    "                self.person_tracks[person_id]['world_landmarks_history'].append(pose['world_landmarks'])\n",
    "            if pose.get('bbox'):\n",
    "                self.person_tracks[person_id]['bbox_history'].append(pose['bbox'])\n",
    "\n",
    "            self.person_tracks[person_id]['last_seen'] = current_time\n",
    "            self.person_tracks[person_id]['total_detections'] += 1\n",
    "\n",
    "        # Clean up old tracks\n",
    "        to_remove = []\n",
    "        for person_id, track in self.person_tracks.items():\n",
    "            if current_time - track['last_seen'] > 5.0:  # Remove after 5 seconds\n",
    "                to_remove.append(person_id)\n",
    "\n",
    "        for person_id in to_remove:\n",
    "            del self.person_tracks[person_id]\n",
    "\n",
    "    def process_frame(self, image):\n",
    "        \"\"\"Process a single frame and return multi-person pose data with improved tracking\"\"\"\n",
    "        self.frame_count += 1\n",
    "        detected_poses = []\n",
    "\n",
    "        if self.use_yolo:\n",
    "            # Use YOLO for person detection\n",
    "            person_boxes = self.detect_persons_yolo(image)\n",
    "\n",
    "            # Detect overlaps\n",
    "            # Pass the list of dictionaries directly to detect_overlaps\n",
    "            overlaps = self.detect_overlaps(person_boxes)\n",
    "\n",
    "            # Process each person\n",
    "            for i, person_box in enumerate(person_boxes):\n",
    "                bbox = person_box['bbox']\n",
    "\n",
    "                # Check if this person is in an overlap group\n",
    "                in_overlap = any(i in group for group in overlaps)\n",
    "\n",
    "                if in_overlap:\n",
    "                    # Use larger ROI for overlapping persons\n",
    "                    roi_image, roi_coords = self.extract_person_roi(image, bbox, padding=40)\n",
    "                else:\n",
    "                    roi_image, roi_coords = self.extract_person_roi(image, bbox)\n",
    "\n",
    "                # Process pose for this person\n",
    "                landmarks = self.process_person_roi(roi_image)\n",
    "\n",
    "                if landmarks:\n",
    "                    try:\n",
    "                        # Transform landmarks back to full image coordinates\n",
    "                        full_landmarks = self.transform_landmarks_to_full_image(\n",
    "                            landmarks, roi_coords, image.shape\n",
    "                        )\n",
    "\n",
    "                        detected_poses.append({\n",
    "                            'landmarks': full_landmarks,\n",
    "                            'world_landmarks': landmarks.world_landmark if hasattr(landmarks, 'world_landmark') else None,\n",
    "                            'bbox': bbox,\n",
    "                            'confidence': person_box['confidence'],\n",
    "                            'in_overlap': in_overlap\n",
    "                        })\n",
    "                    except Exception as e:\n",
    "                        print(f\"⚠️ Landmark transformation failed: {e}\")\n",
    "                        # Fallback: use original landmarks with bbox info\n",
    "                        detected_poses.append({\n",
    "                            'landmarks': landmarks,\n",
    "                            'world_landmarks': landmarks.world_landmark if hasattr(landmarks, 'world_landmark') else None,\n",
    "                            'bbox': bbox,\n",
    "                            'confidence': person_box['confidence'],\n",
    "                            'in_overlap': in_overlap\n",
    "                        })\n",
    "        else:\n",
    "            # Fallback to single person detection\n",
    "            rgb_image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "            results = self.pose.process(rgb_image)\n",
    "\n",
    "            if results.pose_landmarks:\n",
    "                detected_poses.append({\n",
    "                    'landmarks': results.pose_landmarks,\n",
    "                    'world_landmarks': results.pose_world_landmarks if hasattr(results, 'pose_world_landmarks') else None,\n",
    "                    'bbox': None,\n",
    "                    'confidence': 1.0,\n",
    "                    'in_overlap': False\n",
    "                })\n",
    "\n",
    "        # Assign person IDs using Hungarian algorithm\n",
    "        poses_with_ids = self.assign_person_ids(detected_poses)\n",
    "\n",
    "        # Apply temporal smoothing\n",
    "        poses_with_ids = self.apply_advanced_smoothing(poses_with_ids)\n",
    "\n",
    "        # Update tracking history\n",
    "        self.update_tracking_history(poses_with_ids)\n",
    "\n",
    "        # Create a list of dictionaries with 'bbox' keys for detect_overlaps\n",
    "        bboxes_for_overlap = [{'bbox': pose.get('bbox', [0, 0, 1, 1])} for pose in detected_poses if pose.get('bbox')]\n",
    "\n",
    "        return {\n",
    "            'poses': poses_with_ids,\n",
    "            'image_shape': image.shape,\n",
    "            'num_persons_detected': len(poses_with_ids),\n",
    "            'overlaps_detected': len(self.detect_overlaps(bboxes_for_overlap))\n",
    "        }\n",
    "\n",
    "print(\"✅ AdvancedMultiPersonPoseEstimator class created successfully!\")\n",
    "\n",
    "# 3D Pose Estimation and Analysis\n",
    "class Pose3DAnalyzer:\n",
    "    def __init__(self):\n",
    "        self.landmark_names = [\n",
    "            'nose', 'left_eye_inner', 'left_eye', 'left_eye_outer',\n",
    "            'right_eye_inner', 'right_eye', 'right_eye_outer', 'left_ear',\n",
    "            'right_ear', 'mouth_left', 'mouth_right', 'left_shoulder',\n",
    "            'right_shoulder', 'left_elbow', 'right_elbow', 'left_wrist',\n",
    "            'right_wrist', 'left_pinky', 'right_pinky', 'left_index',\n",
    "            'right_index', 'left_thumb', 'right_thumb', 'left_hip',\n",
    "            'right_hip', 'left_knee', 'right_knee', 'left_ankle', 'right_ankle',\n",
    "            'left_heel', 'right_heel', 'left_foot_index', 'right_foot_index'\n",
    "        ]\n",
    "\n",
    "    def extract_3d_coordinates(self, landmarks):\n",
    "        \"\"\"Extract 3D coordinates from MediaPipe landmarks\"\"\"\n",
    "        if not landmarks:\n",
    "            return None\n",
    "\n",
    "        coords_3d = []\n",
    "        for landmark in landmarks.landmark:\n",
    "            coords_3d.append({\n",
    "                'x': landmark.x,\n",
    "                'y': landmark.y,\n",
    "                'z': landmark.z,\n",
    "                'visibility': landmark.visibility\n",
    "            })\n",
    "        return coords_3d\n",
    "\n",
    "    def calculate_angles_3d(self, landmarks):\n",
    "        \"\"\"Calculate 3D angles between key body parts\"\"\"\n",
    "        if not landmarks or len(landmarks.landmark) < 33:\n",
    "            return {}\n",
    "\n",
    "        angles = {}\n",
    "\n",
    "        # Helper function to get 3D vector between two points\n",
    "        def get_vector3d(p1, p2):\n",
    "            return np.array([p2.x - p1.x, p2.y - p1.y, p2.z - p1.z])\n",
    "\n",
    "        # Helper function to calculate angle between two 3D vectors\n",
    "        def angle_between_vectors(v1, v2):\n",
    "            cos_angle = np.dot(v1, v2) / (np.linalg.norm(v1) * np.linalg.norm(v2))\n",
    "            cos_angle = np.clip(cos_angle, -1.0, 1.0)\n",
    "            return np.degrees(np.arccos(cos_angle))\n",
    "\n",
    "        lm = landmarks.landmark\n",
    "\n",
    "        # Elbow angles\n",
    "        if (lm[11].visibility > 0.5 and lm[13].visibility > 0.5 and lm[15].visibility > 0.5):\n",
    "            left_arm_vec1 = get_vector3d(lm[11], lm[13])  # shoulder to elbow\n",
    "            left_arm_vec2 = get_vector3d(lm[13], lm[15])  # elbow to wrist\n",
    "            angles['left_elbow'] = angle_between_vectors(left_arm_vec1, left_arm_vec2)\n",
    "\n",
    "        if (lm[12].visibility > 0.5 and lm[14].visibility > 0.5 and lm[16].visibility > 0.5):\n",
    "            right_arm_vec1 = get_vector3d(lm[12], lm[14])  # shoulder to elbow\n",
    "            right_arm_vec2 = get_vector3d(lm[14], lm[16])  # elbow to wrist\n",
    "            angles['right_elbow'] = angle_between_vectors(right_arm_vec1, right_arm_vec2)\n",
    "\n",
    "        # Knee angles\n",
    "        if (lm[23].visibility > 0.5 and lm[25].visibility > 0.5 and lm[27].visibility > 0.5):\n",
    "            left_leg_vec1 = get_vector3d(lm[23], lm[25])  # hip to knee\n",
    "            left_leg_vec2 = get_vector3d(lm[25], lm[27])  # knee to ankle\n",
    "            angles['left_knee'] = angle_between_vectors(left_leg_vec1, left_leg_vec2)\n",
    "\n",
    "        if (lm[24].visibility > 0.5 and lm[26].visibility > 0.5 and lm[28].visibility > 0.5):\n",
    "            right_leg_vec1 = get_vector3d(lm[24], lm[26])  # hip to knee\n",
    "            right_leg_vec2 = get_vector3d(lm[26], lm[28])  # knee to ankle\n",
    "            angles['right_knee'] = angle_between_vectors(right_leg_vec1, right_leg_vec2)\n",
    "\n",
    "        # Shoulder angles (for shooting stance analysis)\n",
    "        if (lm[11].visibility > 0.5 and lm[12].visibility > 0.5 and lm[23].visibility > 0.5 and lm[24].visibility > 0.5):\n",
    "            shoulder_vec = get_vector3d(lm[11], lm[12])  # left to right shoulder\n",
    "            hip_vec = get_vector3d(lm[23], lm[24])       # left to right hip\n",
    "            angles['shoulder_hip_alignment'] = angle_between_vectors(shoulder_vec, hip_vec)\n",
    "\n",
    "        return angles\n",
    "\n",
    "    def analyze_shooting_stance(self, landmarks):\n",
    "        \"\"\"Analyze shooting stance quality\"\"\"\n",
    "        if not landmarks or len(landmarks.landmark) < 33:\n",
    "            return {}\n",
    "\n",
    "        analysis = {}\n",
    "        lm = landmarks.landmark\n",
    "\n",
    "        # Stance width analysis\n",
    "        if lm[23].visibility > 0.5 and lm[24].visibility > 0.5:\n",
    "            hip_width = abs(lm[24].x - lm[23].x)\n",
    "            analysis['stance_width'] = hip_width\n",
    "            analysis['stance_width_rating'] = 'good' if 0.1 < hip_width < 0.3 else 'needs_adjustment'\n",
    "\n",
    "        # Shoulder alignment\n",
    "        if (lm[11].visibility > 0.5 and lm[12].visibility > 0.5 and\n",
    "            lm[23].visibility > 0.5 and lm[24].visibility > 0.5):\n",
    "            shoulder_center = (lm[11].x + lm[12].x) / 2\n",
    "            hip_center = (lm[23].x + lm[24].x) / 2\n",
    "            alignment_offset = abs(shoulder_center - hip_center)\n",
    "            analysis['shoulder_hip_alignment'] = alignment_offset\n",
    "            analysis['alignment_rating'] = 'good' if alignment_offset < 0.05 else 'needs_adjustment'\n",
    "\n",
    "        # Arm extension for shooting\n",
    "        if (lm[12].visibility > 0.5 and lm[14].visibility > 0.5 and lm[16].visibility > 0.5):\n",
    "            # Calculate arm extension (shoulder to wrist distance)\n",
    "            arm_length = np.sqrt(\n",
    "                (lm[16].x - lm[12].x)**2 +\n",
    "                (lm[16].y - lm[12].y)**2 +\n",
    "                (lm[16].z - lm[12].z)**2\n",
    "            )\n",
    "            analysis['arm_extension'] = arm_length\n",
    "            analysis['extension_rating'] = 'good' if arm_length > 0.3 else 'needs_extension'\n",
    "\n",
    "        # Grip analysis (wrist to hand points)\n",
    "        if (lm[16].visibility > 0.5 and lm[20].visibility > 0.5):\n",
    "            grip_stability = abs(lm[20].x - lm[16].x) + abs(lm[20].y - lm[16].y)\n",
    "            analysis['grip_stability'] = grip_stability\n",
    "            analysis['grip_rating'] = 'stable' if grip_stability < 0.1 else 'unstable'\n",
    "\n",
    "        return analysis\n",
    "\n",
    "    def analyze_martial_arts_stance(self, landmarks):\n",
    "        \"\"\"Analyze martial arts stance quality\"\"\"\n",
    "        if not landmarks or len(landmarks.landmark) < 33:\n",
    "            return {}\n",
    "\n",
    "        analysis = {}\n",
    "        lm = landmarks.landmark\n",
    "\n",
    "        # Center of gravity analysis\n",
    "        if (lm[23].visibility > 0.5 and lm[24].visibility > 0.5 and\n",
    "            lm[25].visibility > 0.5 and lm[26].visibility > 0.5):\n",
    "            # Calculate center of gravity based on hip and knee positions\n",
    "            cog_x = (lm[23].x + lm[24].x + lm[25].x + lm[26].x) / 4\n",
    "            cog_y = (lm[23].y + lm[24].y + lm[25].y + lm[26].y) / 4\n",
    "            analysis['center_of_gravity'] = {'x': cog_x, 'y': cog_y}\n",
    "\n",
    "            # Check if COG is centered between feet\n",
    "            if lm[27].visibility > 0.5 and lm[28].visibility > 0.5:\n",
    "                foot_center_x = (lm[27].x + lm[28].x) / 2\n",
    "                cog_offset = abs(cog_x - foot_center_x)\n",
    "                analysis['cog_balance'] = cog_offset\n",
    "                analysis['balance_rating'] = 'balanced' if cog_offset < 0.1 else 'unbalanced'\n",
    "\n",
    "        # Stance depth (front to back)\n",
    "        if lm[25].visibility > 0.5 and lm[26].visibility > 0.5:\n",
    "            stance_depth = abs(lm[26].y - lm[25].y)\n",
    "            analysis['stance_depth'] = stance_depth\n",
    "            analysis['depth_rating'] = 'good' if 0.05 < stance_depth < 0.2 else 'needs_adjustment'\n",
    "\n",
    "        # Knee bend analysis\n",
    "        angles = self.calculate_angles_3d(landmarks)\n",
    "        if 'left_knee' in angles and 'right_knee' in angles:\n",
    "            knee_bend_avg = (angles['left_knee'] + angles['right_knee']) / 2\n",
    "            analysis['knee_bend'] = knee_bend_avg\n",
    "            analysis['knee_rating'] = 'good' if 120 < knee_bend_avg < 160 else 'needs_adjustment'\n",
    "\n",
    "        return analysis\n",
    "\n",
    "    def detect_hidden_body_parts(self, landmarks, previous_landmarks=None):\n",
    "        \"\"\"Detect and estimate hidden body parts using temporal analysis\"\"\"\n",
    "        if not landmarks or len(landmarks.landmark) < 33:\n",
    "            return {}\n",
    "\n",
    "        hidden_parts = {}\n",
    "        lm = landmarks.landmark\n",
    "\n",
    "        # Analyze visibility of key points\n",
    "        key_points = {\n",
    "            'left_shoulder': 11, 'right_shoulder': 12,\n",
    "            'left_elbow': 13, 'right_elbow': 14,\n",
    "            'left_wrist': 15, 'right_wrist': 16,\n",
    "            'left_hip': 23, 'right_hip': 24,\n",
    "            'left_knee': 25, 'right_knee': 26,\n",
    "            'left_ankle': 27, 'right_ankle': 28\n",
    "        }\n",
    "\n",
    "        for part_name, idx in key_points.items():\n",
    "            if lm[idx].visibility < 0.5:  # Part is hidden or poorly visible\n",
    "                hidden_parts[part_name] = {\n",
    "                    'visibility': lm[idx].visibility,\n",
    "                    'estimated_position': {\n",
    "                        'x': lm[idx].x,\n",
    "                        'y': lm[idx].y,\n",
    "                        'z': lm[idx].z\n",
    "                    },\n",
    "                    'confidence': 'low'\n",
    "                }\n",
    "\n",
    "                # If we have previous landmarks, try to estimate position\n",
    "                if previous_landmarks and len(previous_landmarks.landmark) > idx:\n",
    "                    prev_lm = previous_landmarks.landmark[idx]\n",
    "                    if prev_lm.visibility > 0.5:\n",
    "                        # Use previous position as estimate\n",
    "                        hidden_parts[part_name]['estimated_position'] = {\n",
    "                            'x': prev_lm.x,\n",
    "                            'y': prev_lm.y,\n",
    "                            'z': prev_lm.z\n",
    "                        }\n",
    "                        hidden_parts[part_name]['confidence'] = 'medium'\n",
    "\n",
    "        return hidden_parts\n",
    "\n",
    "print(\"✅ Pose3DAnalyzer class created successfully!\")\n",
    "\n",
    "# Visualization and Analysis Tools\n",
    "class PoseVisualizer:\n",
    "    def __init__(self):\n",
    "        self.colors = [\n",
    "            (255, 0, 0),    # Red\n",
    "            (0, 255, 0),    # Green\n",
    "            (0, 0, 255),    # Blue\n",
    "            (255, 255, 0),  # Yellow\n",
    "            (255, 0, 255),  # Magenta\n",
    "            (0, 255, 255),  # Cyan\n",
    "        ]\n",
    "\n",
    "    def draw_pose_2d(self, image, poses, draw_connections=True, draw_landmarks=True):\n",
    "        \"\"\"Draw 2D pose visualization on image\"\"\"\n",
    "        annotated_image = image.copy()\n",
    "\n",
    "        for i, pose_data in enumerate(poses):\n",
    "            person_id = pose_data['id']\n",
    "            landmarks = pose_data['landmarks']\n",
    "            bbox = pose_data.get('bbox')\n",
    "\n",
    "            # Choose color for this person\n",
    "            color = self.colors[person_id % len(self.colors)]\n",
    "\n",
    "            # Draw bounding box if available\n",
    "            if bbox:\n",
    "                x1, y1, x2, y2 = bbox\n",
    "                cv2.rectangle(annotated_image, (int(x1), int(y1)), (int(x2), int(y2)), color, 2)\n",
    "                cv2.putText(annotated_image, f\"Person {person_id}\", (int(x1), int(y1-10)),\n",
    "                           cv2.FONT_HERSHEY_SIMPLEX, 0.7, color, 2)\n",
    "\n",
    "            if landmarks:\n",
    "                h, w = image.shape[:2]\n",
    "\n",
    "                # Draw landmarks\n",
    "                if draw_landmarks:\n",
    "                    for landmark in landmarks.landmark:\n",
    "                        # Handle both MediaPipe landmarks and simple dict landmarks\n",
    "                        if hasattr(landmark, 'visibility'):\n",
    "                            visibility = landmark.visibility\n",
    "                            x_coord = landmark.x\n",
    "                            y_coord = landmark.y\n",
    "                        else:\n",
    "                            visibility = landmark.get('visibility', 1.0)\n",
    "                            x_coord = landmark.get('x', 0)\n",
    "                            y_coord = landmark.get('y', 0)\n",
    "\n",
    "                        if visibility > 0.5:\n",
    "                            x = int(x_coord * w)\n",
    "                            y = int(y_coord * h)\n",
    "\n",
    "                            cv2.circle(annotated_image, (x, y), 3, color, -1)\n",
    "\n",
    "                # Draw connections\n",
    "                if draw_connections:\n",
    "                    for connection in MARTIAL_ARTS_CONNECTIONS:\n",
    "                        start_idx, end_idx = connection\n",
    "                        if (start_idx < len(landmarks.landmark) and\n",
    "                            end_idx < len(landmarks.landmark)):\n",
    "\n",
    "                            start_lm = landmarks.landmark[start_idx]\n",
    "                            end_lm = landmarks.landmark[end_idx]\n",
    "\n",
    "                            # Handle both MediaPipe landmarks and simple dict landmarks\n",
    "                            if hasattr(start_lm, 'visibility'):\n",
    "                                start_vis = start_lm.visibility\n",
    "                                start_x = start_lm.x\n",
    "                                start_y = start_lm.y\n",
    "                            else:\n",
    "                                start_vis = start_lm.get('visibility', 1.0)\n",
    "                                start_x = start_lm.get('x', 0)\n",
    "                                start_y = start_lm.get('y', 0)\n",
    "\n",
    "                            if hasattr(end_lm, 'visibility'):\n",
    "                                end_vis = end_lm.visibility\n",
    "                                end_x = end_lm.x\n",
    "                                end_y = end_lm.y\n",
    "                            else:\n",
    "                                end_vis = end_lm.get('visibility', 1.0)\n",
    "                                end_x = end_lm.get('x', 0)\n",
    "                                end_y = end_lm.get('y', 0)\n",
    "\n",
    "                            if start_vis > 0.5 and end_vis > 0.5:\n",
    "                                start_x_pixel = int(start_x * w)\n",
    "                                start_y_pixel = int(start_y * h)\n",
    "                                end_x_pixel = int(end_x * w)\n",
    "                                end_y_pixel = int(end_y * h)\n",
    "                                cv2.line(annotated_image, (start_x_pixel, start_y_pixel), (end_x_pixel, end_y_pixel), color, 2)\n",
    "\n",
    "        return annotated_image\n",
    "\n",
    "    def create_3d_plot(self, poses, title=\"3D Pose Visualization\"):\n",
    "        \"\"\"Create 3D plot of pose landmarks\"\"\"\n",
    "        fig = go.Figure()\n",
    "\n",
    "        for i, pose_data in enumerate(poses):\n",
    "            person_id = pose_data['id']\n",
    "            landmarks = pose_data['landmarks']\n",
    "\n",
    "            if landmarks:\n",
    "                # Extract 3D coordinates\n",
    "                x_coords = [lm.x for lm in landmarks.landmark]\n",
    "                y_coords = [lm.y for lm in landmarks.landmark]\n",
    "                z_coords = [lm.z for lm in landmarks.landmark]\n",
    "                visibility = [lm.visibility for lm in landmarks.landmark]\n",
    "\n",
    "                # Filter visible points\n",
    "                visible_x = [x for x, v in zip(x_coords, visibility) if v > 0.5]\n",
    "                visible_y = [y for y, v in zip(y_coords, visibility) if v > 0.5]\n",
    "                visible_z = [z for z, v in zip(z_coords, visibility) if v > 0.5]\n",
    "\n",
    "                # Add scatter plot for landmarks\n",
    "                fig.add_trace(go.Scatter3d(\n",
    "                    x=visible_x,\n",
    "                    y=visible_y,\n",
    "                    z=visible_z,\n",
    "                    mode='markers',\n",
    "                    marker=dict(\n",
    "                        size=5,\n",
    "                        color=self.colors[person_id % len(self.colors)],\n",
    "                        opacity=0.8\n",
    "                    ),\n",
    "                    name=f'Person {person_id}',\n",
    "                    text=[f'Landmark {j}' for j in range(len(visible_x))],\n",
    "                    hovertemplate='<b>%{text}</b><br>X: %{x:.3f}<br>Y: %{y:.3f}<br>Z: %{z:.3f}<extra></extra>'\n",
    "                ))\n",
    "\n",
    "                # Add connections\n",
    "                for connection in MARTIAL_ARTS_CONNECTIONS:\n",
    "                    start_idx, end_idx = connection\n",
    "                    if (start_idx < len(landmarks.landmark) and\n",
    "                        end_idx < len(landmarks.landmark)):\n",
    "\n",
    "                        start_lm = landmarks.landmark[start_idx]\n",
    "                        end_lm = landmarks.landmark[end_idx]\n",
    "\n",
    "                        if start_lm.visibility > 0.5 and end_lm.visibility > 0.5:\n",
    "                            fig.add_trace(go.Scatter3d(\n",
    "                                x=[start_lm.x, end_lm.x],\n",
    "                                y=[start_lm.y, end_lm.y],\n",
    "                                z=[start_lm.z, end_lm.z],\n",
    "                                mode='lines',\n",
    "                                line=dict(\n",
    "                                    color=self.colors[person_id % len(self.colors)],\n",
    "                                    width=3\n",
    "                                ),\n",
    "                                showlegend=False,\n",
    "                                hoverinfo='skip'\n",
    "                            ))\n",
    "\n",
    "        fig.update_layout(\n",
    "            title=title,\n",
    "            scene=dict(\n",
    "                xaxis_title='X',\n",
    "                yaxis_title='Y',\n",
    "                zaxis_title='Z',\n",
    "                aspectmode='data'\n",
    "            ),\n",
    "            width=800,\n",
    "            height=600\n",
    "        )\n",
    "\n",
    "        return fig\n",
    "\n",
    "    def create_analysis_dashboard(self, poses, analysis_data):\n",
    "        \"\"\"Create analysis dashboard with multiple plots\"\"\"\n",
    "        if not poses:\n",
    "            return None\n",
    "\n",
    "        # Create subplots\n",
    "        fig = make_subplots(\n",
    "            rows=2, cols=2,\n",
    "            subplot_titles=('Pose Analysis', '3D Visualization', 'Angles', 'Stance Quality'),\n",
    "            specs=[[{\"type\": \"scatter\"}, {\"type\": \"scatter3d\"}],\n",
    "                   [{\"type\": \"bar\"}, {\"type\": \"bar\"}]]\n",
    "        )\n",
    "\n",
    "        # Add pose analysis (placeholder for now)\n",
    "        fig.add_trace(\n",
    "            go.Scatter(x=[0, 1], y=[0, 1], mode='markers', name='Pose Points'),\n",
    "            row=1, col=1\n",
    "        )\n",
    "\n",
    "        # Add 3D visualization\n",
    "        for i, pose_data in enumerate(poses):\n",
    "            landmarks = pose_data['landmarks']\n",
    "            if landmarks:\n",
    "                x_coords = [lm.x for lm in landmarks.landmark if lm.visibility > 0.5]\n",
    "                y_coords = [lm.y for lm in landmarks.landmark if lm.visibility > 0.5]\n",
    "                z_coords = [lm.z for lm in landmarks.landmark if lm.visibility > 0.5]\n",
    "\n",
    "                fig.add_trace(\n",
    "                    go.Scatter3d(\n",
    "                        x=x_coords, y=y_coords, z=z_coords,\n",
    "                        mode='markers',\n",
    "                        name=f'Person {pose_data[\"id\"]}',\n",
    "                        marker=dict(size=3)\n",
    "                    ),\n",
    "                    row=1, col=2\n",
    "                )\n",
    "\n",
    "        # Add angles analysis\n",
    "        if 'angles' in analysis_data:\n",
    "            angles = analysis_data['angles']\n",
    "            angle_names = list(angles.keys())\n",
    "            angle_values = list(angles.values())\n",
    "\n",
    "            fig.add_trace(\n",
    "                go.Bar(x=angle_names, y=angle_values, name='Joint Angles'),\n",
    "                row=2, col=1\n",
    "            )\n",
    "\n",
    "        # Add stance quality\n",
    "        if 'stance_analysis' in analysis_data:\n",
    "            stance = analysis_data['stance_analysis']\n",
    "            metrics = list(stance.keys())\n",
    "            values = [stance[m] if isinstance(stance[m], (int, float)) else 0 for m in metrics]\n",
    "\n",
    "            fig.add_trace(\n",
    "                go.Bar(x=metrics, y=values, name='Stance Metrics'),\n",
    "                row=2, col=2\n",
    "            )\n",
    "\n",
    "        fig.update_layout(\n",
    "            title=\"Pose Analysis Dashboard\",\n",
    "            height=800,\n",
    "            showlegend=True\n",
    "        )\n",
    "\n",
    "        return fig\n",
    "\n",
    "print(\"✅ PoseVisualizer class created successfully!\")\n",
    "\n",
    "# Video Processing and Testing\n",
    "class VideoProcessor:\n",
    "    def __init__(self, pose_estimator, pose_analyzer, visualizer):\n",
    "        self.pose_estimator = pose_estimator\n",
    "        self.pose_analyzer = pose_analyzer\n",
    "        self.visualizer = visualizer\n",
    "        self.frame_count = 0\n",
    "        self.results_history = []\n",
    "\n",
    "    def process_video_file(self, video_path, output_path=None, max_frames=None):\n",
    "        \"\"\"Process a video file and return results\"\"\"\n",
    "        cap = cv2.VideoCapture(video_path)\n",
    "\n",
    "        if not cap.isOpened():\n",
    "            print(f\"Error: Could not open video file {video_path}\")\n",
    "            return None\n",
    "\n",
    "        fps = int(cap.get(cv2.CAP_PROP_FPS))\n",
    "        total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "        width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "        height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "\n",
    "        print(f\"Video Info: {width}x{height}, {fps} FPS, {total_frames} frames\")\n",
    "\n",
    "        # Setup video writer if output path is provided\n",
    "        out = None\n",
    "        if output_path:\n",
    "            fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
    "            out = cv2.VideoWriter(output_path, fourcc, fps, (width, height))\n",
    "\n",
    "        frame_results = []\n",
    "        frame_count = 0\n",
    "\n",
    "        while True:\n",
    "            ret, frame = cap.read()\n",
    "            if not ret:\n",
    "                break\n",
    "\n",
    "            if max_frames and frame_count >= max_frames:\n",
    "                break\n",
    "\n",
    "            # Process frame\n",
    "            start_time = time.time()\n",
    "            result = self.pose_estimator.process_frame(frame)\n",
    "            processing_time = time.time() - start_time\n",
    "\n",
    "            # Analyze poses\n",
    "            analysis_results = []\n",
    "            for pose_data in result['poses']:\n",
    "                landmarks = pose_data['landmarks']\n",
    "                if landmarks:\n",
    "                    # 3D analysis\n",
    "                    coords_3d = self.pose_analyzer.extract_3d_coordinates(landmarks)\n",
    "                    angles = self.pose_analyzer.calculate_angles_3d(landmarks)\n",
    "                    shooting_analysis = self.pose_analyzer.analyze_shooting_stance(landmarks)\n",
    "                    martial_arts_analysis = self.pose_analyzer.analyze_martial_arts_stance(landmarks)\n",
    "                    hidden_parts = self.pose_analyzer.detect_hidden_body_parts(landmarks)\n",
    "\n",
    "                    analysis_results.append({\n",
    "                        'person_id': pose_data['id'],\n",
    "                        'coords_3d': coords_3d,\n",
    "                        'angles': angles,\n",
    "                        'shooting_analysis': shooting_analysis,\n",
    "                        'martial_arts_analysis': martial_arts_analysis,\n",
    "                        'hidden_parts': hidden_parts\n",
    "                    })\n",
    "\n",
    "            # Draw visualization\n",
    "            annotated_frame = self.visualizer.draw_pose_2d(frame, result['poses'])\n",
    "\n",
    "            # Add processing info\n",
    "            cv2.putText(annotated_frame, f\"Frame: {frame_count}\", (10, 30),\n",
    "                       cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 255, 255), 2)\n",
    "            cv2.putText(annotated_frame, f\"Persons: {len(result['poses'])}\", (10, 70),\n",
    "                       cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 255, 255), 2)\n",
    "            cv2.putText(annotated_frame, f\"Time: {processing_time:.3f}s\", (10, 110),\n",
    "                       cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 255, 255), 2)\n",
    "\n",
    "            # Write frame if output is specified\n",
    "            if out:\n",
    "                out.write(annotated_frame)\n",
    "\n",
    "            # Store results\n",
    "            frame_results.append({\n",
    "                'frame_number': frame_count,\n",
    "                'poses': result['poses'],\n",
    "                'analysis': analysis_results,\n",
    "                'processing_time': processing_time,\n",
    "                'timestamp': frame_count / fps\n",
    "            })\n",
    "\n",
    "            frame_count += 1\n",
    "\n",
    "            # Print progress\n",
    "            if frame_count % 30 == 0:\n",
    "                print(f\"Processed {frame_count}/{total_frames} frames\")\n",
    "\n",
    "        cap.release()\n",
    "        if out:\n",
    "            out.release()\n",
    "\n",
    "        print(f\"Video processing complete. Processed {frame_count} frames.\")\n",
    "        return frame_results\n",
    "\n",
    "    def process_webcam(self, duration=10):\n",
    "        \"\"\"Process webcam feed for real-time testing\"\"\"\n",
    "        cap = cv2.VideoCapture(0)\n",
    "\n",
    "        if not cap.isOpened():\n",
    "            print(\"Error: Could not open webcam\")\n",
    "            return\n",
    "\n",
    "        start_time = time.time()\n",
    "        frame_count = 0\n",
    "\n",
    "        print(\"Starting webcam processing. Press 'q' to quit.\")\n",
    "\n",
    "        while True:\n",
    "            ret, frame = cap.read()\n",
    "            if not ret:\n",
    "                break\n",
    "\n",
    "            # Check duration\n",
    "            if time.time() - start_time > duration:\n",
    "                break\n",
    "\n",
    "            # Process frame\n",
    "            result = self.pose_estimator.process_frame(frame)\n",
    "\n",
    "            # Draw visualization\n",
    "            annotated_frame = self.visualizer.draw_pose_2d(frame, result['poses'])\n",
    "\n",
    "            # Add info\n",
    "            cv2.putText(annotated_frame, f\"Frame: {frame_count}\", (10, 30),\n",
    "                       cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 255, 255), 2)\n",
    "            cv2.putText(annotated_frame, f\"Persons: {len(result['poses'])}\", (10, 70),\n",
    "                       cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 255, 255), 2)\n",
    "\n",
    "            # Show frame\n",
    "            cv2.imshow('Pose Estimation', annotated_frame)\n",
    "\n",
    "            # Break on 'q' key\n",
    "            if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "                break\n",
    "\n",
    "            frame_count += 1\n",
    "\n",
    "        cap.release()\n",
    "        cv2.destroyAllWindows()\n",
    "        print(f\"Webcam processing complete. Processed {frame_count} frames.\")\n",
    "\n",
    "    def create_sample_video(self, output_path=\"sample_poses.mp4\", duration=5):\n",
    "        \"\"\"Create a sample video with pose estimation for testing\"\"\"\n",
    "        # Create a simple test video with moving rectangles (simulating people)\n",
    "        fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
    "        out = cv2.VideoWriter(output_path, fourcc, 30, (640, 480))\n",
    "\n",
    "        for frame_num in range(duration * 30):\n",
    "            # Create a black frame\n",
    "            frame = np.zeros((480, 640, 3), dtype=np.uint8)\n",
    "\n",
    "            # Add some moving rectangles to simulate people\n",
    "            x1 = int(100 + 50 * np.sin(frame_num * 0.1))\n",
    "            y1 = int(200 + 30 * np.cos(frame_num * 0.1))\n",
    "            x2 = int(400 + 40 * np.sin(frame_num * 0.15))\n",
    "            y2 = int(150 + 25 * np.cos(frame_num * 0.12))\n",
    "\n",
    "            cv2.rectangle(frame, (x1, y1), (x1+100, y1+200), (255, 255, 255), -1)\n",
    "            cv2.rectangle(frame, (x2, y2), (x2+100, y2+200), (255, 255, 255), -1)\n",
    "\n",
    "            out.write(frame)\n",
    "\n",
    "        out.release()\n",
    "        print(f\"Sample video created: {output_path}\")\n",
    "\n",
    "print(\"✅ VideoProcessor class created successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "815f7ff4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the complete system\n",
    "print(\"🚀 Initializing Multi-Person 3D Pose Estimation System...\")\n",
    "\n",
    "# Create configuration\n",
    "config = PoseConfig()\n",
    "\n",
    "# Initialize components\n",
    "pose_estimator = AdvancedMultiPersonPoseEstimator(config)\n",
    "pose_analyzer = Pose3DAnalyzer()\n",
    "visualizer = PoseVisualizer()\n",
    "video_processor = VideoProcessor(pose_estimator, pose_analyzer, visualizer)\n",
    "\n",
    "print(\"✅ System initialized successfully!\")\n",
    "print(f\"📊 Configuration:\")\n",
    "print(f\"   - Max persons: {config.MAX_PERSONS}\")\n",
    "print(f\"   - Model complexity: {config.MEDIAPIPE_MODEL_COMPLEXITY}\")\n",
    "print(f\"   - 3D estimation: {config.ENABLE_3D_ESTIMATION}\")\n",
    "print(f\"   - YOLO enabled: {pose_estimator.use_yolo}\")\n",
    "print(f\"   - Smoothing window: {config.SMOOTHING_WINDOW_SIZE}\")\n",
    "\n",
    "# Test with a sample image\n",
    "print(\"\\n🧪 Testing with sample image...\")\n",
    "\n",
    "# Create a test image\n",
    "test_image = np.zeros((480, 640, 3), dtype=np.uint8)\n",
    "cv2.rectangle(test_image, (100, 100), (300, 400), (255, 255, 255), -1)\n",
    "cv2.rectangle(test_image, (400, 150), (600, 450), (255, 255, 255), -1)\n",
    "\n",
    "# Process the test image\n",
    "result = pose_estimator.process_frame(test_image)\n",
    "print(f\"✅ Test completed. Detected {len(result['poses'])} persons.\")\n",
    "\n",
    "# Display system status\n",
    "print(f\"\\n📈 System Status:\")\n",
    "print(f\"   - Pose estimator: ✅ Ready\")\n",
    "print(f\"   - 3D analyzer: ✅ Ready\") \n",
    "print(f\"   - Visualizer: ✅ Ready\")\n",
    "print(f\"   - Video processor: ✅ Ready\")\n",
    "print(f\"   - Person tracks: {len(pose_estimator.person_tracks)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5e5a042",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
